\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{makecell, multirow}

% \usepackage{fontspec}

% Load Fonts
% \setromanfont[
% BoldFont=timesb.ttf,
% ItalicFont=timesi.ttf,
% BoldItalicFont=timesbi.ttf,
% ]{times.ttf}
% \setsansfont[
% BoldFont=arialbd.ttf,
% ItalicFont=ariali.ttf,
% BoldItalicFont=arialbi.ttf
% ]{arial.ttf}
% \setmonofont[Scale=0.90,
% BoldFont=courierb.ttf,
% ItalicFont=courieri.ttf,
% BoldItalicFont=courierbi.ttf,
% Color={0019D4}
% ]{courier.ttf}

\doublespacing
\title{Target-Tracking Using Neural Networks on Low-Power Devices with Applications to UAVs}
\author{Ian Riley}
\date{December 2016}

\begin{document}
\pagenumbering{roman}
\maketitle
\thispagestyle{empty}

\newpage
{
\setstretch{1.0}
\begin{tabularx}{\textwidth}{lX}
\textbf{Thesis} & Target-Tracking Using Neural Networks on Low-Power Devices with Applications to UAVs \\
& \\
\textbf{Author} & Ian Drea Riley \\
& \\
\textbf{Term Submitted} & Fall 2017 \\
& \\
Dr. Amar Raheja & \\
Thesis Committee Chair & \\
Department of Computer Science & \hrulefill \\
& \\
Dr. Subodh Bhandari & \\
Department of Aerospace Engineering & \hrulefill \\
& \\
Dr. Hao Ji & \\
Department of Computer Science & \hrulefill \\
\end{tabularx}
}

\newpage
\newgeometry{left=1.5in, top=1in, right=1in, bottom=1in}
\section*{Acknowledgements}
Completing this thesis would have been impossible were it not for the tremendous good fortune I've had to be surrounded by loving, intelligent, and supportive people. To Mom and Dad, first of all, for supporting your decreasingly youthful son while he played with computers into his mid-thirties. To Alexandra, for listening to me talk about computational graphs with something that passed for interest; you are first and last in my heart. To Devin, for the soundest career advice; wubba-wubba. To
Dr. Amar Raheja, for your ongoing academic and professional support; I owe you a debt of gratitude too large to repay here. To Dr. Subodh Bhandari, for teaching me to google. To Arvin, Joshua, Alex, Fernando, Miguel, and Rene; a finer group of aerospace engineers was never assembled. To Luca Bertinetto and Jack Valmadre, for publishing a fantastically elegant tracking algorithm, and then answering my thousand questions about how it works. To Jim at jetsonhacks.com, for rad technical support
and radder jam sessions. To my friends, for giving me the thumbs up even when they probably shouldn't have.
\newpage
\begin{abstract}
Recent advances in computer vision and general purpose graphic processing units (GPUs) have made the possibility of small, intelligent, vision-based robots a reality. In this research, I focus specifically on the problem of autonomous target-tracking by small, low-power devices.
Automated target-tracking has long been a problem of interest in the field of computer vision. This is motivated by the fact that visual surveillance is a simple and tedious task. Yet truly robust solutions for visual target tracking have been illusive. The problem at the center of persistent target tracking, is how to make the tracking program robust to changes in the target's appearance. An ideal tracker would be robust to changes in object scale and rotation, changes in lighting,
object deformation and full- or partial-occlusion. Recent advances in object classification using convolutional neural networks (CNNs) have made them popular for use in anything computer-vision related. Until recently a lack of sufficient labeled video data limited attempts at using CNNs for target tracking to hacking the features of trained object classifiers for spatial data, operations that are too expensive for the real-time requirements of target trackers. However,
\cite{bertinetto2016fully} uses the recently released ImageNet video dataset to train a fully-convolutional network that is capable of tracking with state-of the art accuracy, at more than 10 times the speed of some CNN-based trackers.
At the same time, computer hardware has become cheaper, faster, and increasingly focused on parallel processing. Because neural-network architecture is meant to mimic the highly parallel architecture of the brain, they pair naturally with massively parallel computer architectures like those of GPUs, which often have hundreds, if not thousands, of small, low-speed cores. The advance of GPU architecture has made running neural-networks at real-time speeds achievable. More recent
GPUs have been designed specifically with low-power devices in mind. While they do not deliver the computational power of larger, high-power GPUs, they make highly-parallel processing available to battery-operated devices.
In this research, we attempt to show that a real-time target-tracking software can be run on a small, low-power GPU that could feasibly be installed as a companion computer to a small robot, and give it target-tracking capabilities.
\end{abstract}
\newpage
\tableofcontents

\newpage
\pagenumbering{arabic}

\section{Introduction}

Over the past decade, small unmanned aerial vehicles (UAVs) have drawn increasing interest for their potential range of utility in civilian, industrial and military applications. Rotary wing UAVs have received special attention. Their ability to take-off and land vertically, hover and reduce airspeed arbitrarily without stalling makes them particularly well suited to navigate several otherwise difficult but interesting environments such as indoor environments, cities, and any outdoor environment that has constrained spaces or a preponderance of physical obstacles.

During the same time period, there has been an increased interest in autonomous robotics. Improved artificial intelligence (AI) algorithms have made the possibility of robots autonomously carrying out helpful tasks a viable one; a fact which has not gone unnoticed by commercial and defense industries. Not surprisingly, there have been several attempts to make UAVs capable of performing various flight missions autonomously; these include flight control \cite{zhang2015autonomous}, cargo delivery \cite{zhao2015robust}, and target tracking \cite{lin2012robust,boudjit2015detection,woods2015dynamic,pestana2014computer}.

Target tracking is of particular interest because it generalizes other more specific problems like runway approach, and cargo pickup / delivery. It also has obvious applications in such disparate areas as surveillance, wildlife observation, and sports cinematography.

Target-tracking by itself has proven to be a difficult computer vision task, and is made even more difficult when constrained by real-time performance, as is necessary for autonomous target-tracking. Until recently, the target-tracking problem has been dominated by feature-based algorithms that build models of the object to be tracked \cite{kalal2012tracking,Nebehay2015CVPR}. These models, however, are often not robust to severe changes in the appearance of the target, and their models tend to drift. The advent of convolutional neural networks (CNNs) for computer vision applications in recent years has given rise to several new, deep-learning-based approaches to the target tracking problem. Within the last 5 years, convolutional neural networks (CNNs) have produced outstanding results in the field of image recognition, both in classification and object localization \cite{krizhevsky2012imagenet,simonyan2014very,szegedy2015going,he2015deep}. CNNs have been shown to successfully extract high-level features directly from raw data, which avoids the potential for logical errors in hand-coded features.
Not surprisingly, CNNs have also been employed in the domain of object identification and tracking in video with some success\cite{redmon2015you,ning2016spatially,he2015deep,li2016deeptrack,wang2013learning,wang2015visual,ma2015hierarchical,nam2015learning,bertinetto2016fully}. Object detection and tracking in video poses problems additional to those posed by simple object classification and localization - notably to "remember" a particular target when it is occluded for a span of time (e.g. a car going under an overpass) or if it momentarily leaves the camera frame, or when there are several objects in view that are similar to the target. Although there is some recent evidence that the use of recurrent neural networks (RNNs) in tandem with the features extracted by CNNs may make CNN based trackers more robust to gaps in target visibility \cite{ning2016spatially}.

Given the clear utility of a drone capable of visually tracking a target, little research appears to have been done towards a viable, robust, and onboard solution to the target tracking problem, and what research there is makes no use of deep-learning-based algorithms. Further, several of the algorithms \cite{pestana2014computer, woods2015dynamic} do all computer-vision related tasks on a remote computer (i.e., not onboard) which places a significant constraint on the UAV, since it must always maintain a line of communication with the remote.

In this thesis, I present two implementations of an onboard target-tracking module, one using a feature-based detection algorithm, and another using a CNN-based detection algorithm, and compare their results. I find that while the CNN-based tracker outperforms the feature-based tracker by a wide margin, its frame-rate is too slow to meet real-time requirements.

%Target-tracking using CNNs, then, seems like a promising route for developing the tracking capacity necessary for autonomous drone surveillance. However, the high computational cost of CNNs as compared to other tracking methods poses a challenge in the realm of real-time target tracking. General Purpose Graphic Processing Units (GPGPUs) are used to reduce the compute-time required for prediction using CNNs, however only a very few architectures have managed the speed required for real-time tracking.

%A further complication is that small drones operate on very small power supplies. The fastest GPGPUs, which are often the test hardware for the previously mentioned target-tracking architectures, have TDPs of around 250 watts - far too much power for a quad-copter to deliver on its limited power supply. Recent GPGPU designs geared specifically for low-power applications have made it possible to run complex parallel applications like CNNs in low-power environments, like a small battery-powered drone. Although they do not provide as much computational power as their larger, more power-hungry counterparts, it is our hope that I may successfully run neural-network-based target tracking software on these modules, in flight. 

\section{Literature Review}

\subsection{UAV target tracking}

Research on monocular, vision-based target tracking for UAVs appears sparse, despite continuing interest and advances in vision-based target tracking. For small, rotary wing drones I know of only a few previous attempts to automate target tracking \cite{woods2015dynamic,lin2012robust,boudjit2015detection,pestana2014computer}.

In \cite{lin2012robust}, which is the work most closely related to this research, Feng Lin uses an onboard CPU on a small unmanned helicopter to do all computer vision processing for the purpose of target tracking. Object detection is accomplished through a multistep process of color thresholding, morphological augmentation, and Kalman filtering. The image is converted from HSV format to binary format by thresholding the HSV image against a preset range of colors. All pixels that do not fall in the range are assigned a zero in the binary image; all other pixels are assigned a one. Next, the pixels of interest in the binary image (ones) are grouped into regions of interest (ROIs). Kalman filtering is then used to add dynamic features to the object. A bayesian discriminant filter is applied to the features of each ROI to pick one as the new target position.

However, for this tracking algorithm to work, it makes several strong assumptions about the target. First, it assumes that the target is \emph{Lambertian Reflective}, that is, that it has an ideally \emph{matte} surface, meaning its brightness doesn't change with the angle its viewed from. Second, it assumes that the color distribution of the object is distinct from the color distribution of its surroundings. Third, and perhaps most restrictive, the target must be chosen from a predefined set of targets. While this is a good step in the direction of tracking autonomy, it is clearly too limited for general-purpose tracking. The first two constraints suggest the tracker could be easily fooled by reflective, luminous, and camouflaged objects, and the restriction of targets to a predefined list means that a database would have to be updated before tracking could begin. However, humans have no such restrictions on their ability to follow objects visually. In order for target tracking to be truly autonomous, it must be as good as or better than human performance.

Jesus Pestana uses a much more robust tracker in his work in \cite{pestana2014computer}. The tracker, dubbed TLD \cite{kalal2012tracking}, uses both features and deep learning for object detection, and is discussed further in section \ref{feat_tracking}. TLD is not subject to the constraints in Lin's algorithm, which makes the results of Pestana's work much more compelling. However, Pestana's work relies on a remote computer to do all image processing, which restricts the use of the drone to areas where it can maintain constant communication with the remote.

The work in \cite{woods2015dynamic,boudjit2015detection} both use remote computers for image processing as well and, in the case of \cite{woods2015dynamic} an external sensor system as well. These works certainly do important work to demonstrate the feasibility of computer-vision-based control logic, however, they cannot be considered for broad practical applications because they are tethered to external equipment.

Among these works, only \cite{lin2012robust} uses an onboard target tracker; the rest handle tracking and control logic on a remote computer that the drone communicates with via wifi. None of the cited works use neural-network-based trackers. To the best of my knowledge, there is no existing work that attempts to use deep-learning-based computer vision for onboard target tracking. This is likely due to the absence, until very recently, of computer architectures that were simultaneously, small, low-power, massively parallel and fast. Small copter drones can only carry very small payloads. The hexcopter drone used for this research required that the payload be less than 5 oz. Recent advances in computer hardware, notably the availability of low-power GPUs, allow for a computer to be carried on-board a small drone. Further, these small, highly parallel modules do not draw much power, so they do not require drones to carry larger batteries, and can compute large amounts of data in parallel, which means that neural networks can run quickly on them.

One of the major difficulties posed by monocular target tracking in the domain of drone control is range estimation. Since the drone itself is presumed to be moving, and has real physical boundaries, it is important that the drone keep a safe distance from the target, or approach it at a reasonable speed. Because a pinhole camera provides a single 2D projection of a 3D space, range cannot be calculated directly, but must be estimated from the
image. However \cite{dobrokhodov2006vision} shows that, if the target can be assumed to be a land vehicle, range estimation is not particularly difficult; although it is worth noting, that estimation noise is less tolerable as the UAV approaches the object, as in the case of making a landing.

\subsection{Object Tracking}

In the broadest terms, object tracking can be defined as follows: Given a sequence of images and the target object's position in the first image, return the target position for all subsequent images. For the purposes of this paper, we use the definition implied by the Online Tracking Benchmark \cite{wu2013online}: The position of a target is given by a polygonal region in the first image of a sequence, and the tracking function must return a bounding box (i.e. a rectangle normal to the axes of the image) for all subsequent frames that tightly matches the bounding box drawn by a human control.

A wide variety of approaches have been taken to the tracking problem, most of them depending on a model of the target built during training. More recently, deep-learning algorithms have been successfully incorporated into tracking algorithms to make the models more robust to holes in image-processing logic.

In even the most mundane circumstances, trackers must overcome a wide variety of obstacles in order to perform well. Indeed, \cite{wu2013online} defines eleven different "attributes" of video sequences that can impede tracking performance. Among these are occlusion, rotation both in and out of the image plane, and background clutter. To accomplish this, the tracker must often build a model of the target that incorporates several views of the target. However, a dynamic model introduces the possibility that the model can drift, meaning it starts to incorporate background information until it loses the target completely. In this sense, the target tracking problem is largely one of creating a algorithm that is robust to changes in target appearance while eliminating drift.

\subsubsection{Feature-based methods}\label{feat_tracking}

So-called "feature-based" methods of object tracking rely on image features computed by various image processing techniques (e.g. edges, histograms, optical flow) to generate models of the targeted object and of the background. Using these models, logic is implemented to find the object in each frame.

Two major non-NN approaches have been taken to visual object-tracking using hand-crafted representations: Generative and Discriminative. Generative models attempt to use known occurrences of the object in past frames to generate an internal representation of the object. How this representation is built is the major issue: It must be robust enough that the object is not lost in the background - but it also must be flexible enough to tolerate object
deformation, and changes in view. A recent example of a high performing generative model is in \cite{Nebehay2015CVPR}.

Discriminative models, on the other hand, attempt to generate model of the background, seek to distinguish the object by discriminating it from its background.

Other models, like the TLD model describe in \cite{kalal2012tracking}, a hybrid approach. In the case of TLD, two critic networks are trained estimate negative and positive instances of the target from the video sequence. This is then used to train an SVM to distinguish the target from its background.

\subsubsection{Neural-Network methods}

However, in recent years, the best deep-learning trackers have far outstripped non-CNN trackers. However, target tracking is a significantly more challenging task than object recognition, especially due the the time-constraints required by the speed of video sequences. Several approaches have been taken to incorporate CNNs effectively into target-tracking programs. Perhaps the simplest approach is described in \cite{redmon2015you}. Tracking in this
case is reduced to object classification and localization performed at real-time speed. It's worth noting that this approach is not a fully fledged target tracker, in that the CNN generates a bounding box for every object that surpasses a given threshold on any of the object classes. There is no way to pick a specific target to the exclusion of others. However, the work offers an important insight: that using a regression strategy for localizing
objects is far more efficient than using region proposal networks, and performs almost as well. This results in a CNN that can operate at real-time speed with very good performance, while other CNN models for localization are far too slow (on even the fastest hardware) to perform in real-time (which I define here as $\ge 20$ fps). The work in \cite{ning2016spatially} improves on \cite{redmon2015you} by using a recurrent neural network
(RNN) to track the movement of objects through time. This is an intriguing direction because, unlike the following methods, the RNN is trained to recognize sequential image patterns (that is, the possible transformations an object can take given a current position).

\cite{wang2013learning} uses a deep auto-encoder to learn image features offline. Targets are then learned on-line using the features extracted from images by the auto-encoder. \cite{li2016deeptrack} trains an ensemble of CNNs on-line, starting with an initial known instance of the target, to differentiate the target from the background. \cite{hong2015online} uses the image features from a pretrained CNN to train a simple classifier (SVM) on-line to
create a model of the target. Interestingly, Hong uses "deconvolution" to create a heat-map out of the input frame. The SVM then uses the heat-map to learn to find the target. So-called deconvolution is the process of back-tracking through the convolutions to find out which input data (pixels) most "excited" the neuron responsible for identifying the target. The work in \cite{wang2015visual} and \cite{ma2015hierarchical} actually seeks to discover the
utility of each layer of a pre-trained CNN, and uses that knowledge to make a more informed model of the target. They show that certain convolutional layers encode views of the target from different perspectives, while other layers encode meaning like categorization. From these layers they develop a model of the target, and use it to track the target with high accuracy. A more complex approached, used in \cite{nam2015learning}, is to train
domain-specific layers to the CNN that handle specific challenges (e.g. object rotation, object occlusion) in target-tracking. While complicated, the results appear more robust across various challenges.

Recently, the availability of the large ImageNet 2015 object-tracking video dataset has allowed for proper training of deep-learning based target-trackers. \cite{bertinetto2016fully} shows that, with this large amount of training data, a simple "Siamese" network can be trained to successfully track an object, without resorting to computationally expensive methods like back propagation or online learning. A Siamese network, simply, is one that
passes the input through a neural network, and compares the output to the output generated by a separate image on the same neural network. If the outputs are considered similar enough, the network predicts that they are the same object, otherwise, it predicts that they are different objects. Despite the simplicity of the approach in \cite{bertinetto2016fully}, the resulting target tracker achieves state-of-the-art performance
and has an fps that more than 10 times faster than some other CNN trackers.

\subsection{Convolutional Neural Networks and Computer Vision}
As noted above, CNNs have, in recent years, proved adept at image classification. The CNN architecture is a special case of the more general "feed-forward" architecture.
\subsubsection{Feed-Forward Neural Networks}
Feed-forward neural networks can be represented as an acyclic computational graph composed of "layers" of vertices, where each vertex is connected to any number of vertices in the layers adjacent to it, but not connected to any of the vertices in its own layer. Each edge in the graph has an associated weight, and the output of each vertex is the sum of the weighted outputs of the previous layer's vertices passed through some nonlinear function.

Perhaps the simplest of these architectures is a "fully connected" neural network, in which each neuron (vertex) is connected to all neurons in the prior and subsequent layers. A small instance is shown in figure  \ref{fig:fcnn}. Indeed, fully connected neural networks have a wonderfully simple architecture when compared with CNNs and, for a similar number of parameters (edges), compute a forward pass significantly faster. However, the number of parameters in a
fully connected neural networks grows very quickly with the size of the input ($O(n)^2$). For large and deep networks, the number of parameters becomes too large to keep in memory, and computation slows drastically. The CNN architecture reduces the total number of parameters by "sharing" them across neurons in a given layer. It turns out this method also preserves the spatial locality of image features, which is of obvious value in object localization and
classification.

\begin{figure}[h]
\includegraphics[width=\linewidth]{img/FCNN.png}
\caption{A fully connected neural network}
\label{fig:fcnn}
\end{figure}

\subsubsection{Convolutional Neural Network Architecture}
The architecture of fully connected networks makes the naive assumption that any number of inputs may have a relationship with each other; that is, that, combined, they define a feature. In the image domain, where each element of the input vector is a pixel, this is almost never the case. It is far more likely that two adjacent pixels are related to each other (for example, are part of the same object) than it is that pixels on opposite edges of
the image are. CNNs make use of this spatial relationship by grouping weights together into small, square \emph{filters} (also called \emph{kernels}) that are passed over the image in discrete steps. Each step represents a neuron in the next layer. So the weights in each filter are shared across neurons. This drastically reduces the number of parameters necessary for a network with large input, and uses the parameters more efficiently. This is especially true because the early layers of a
deep neural network end up tasked with extracting low-level features - like straight lines and areas of intense color. These features are common throughout images, so it makes sense to use a single set of weights to look for them rather than an individual set for each area in the image.

More formally, let $F$ and $I$ be two two-dimensional tensors of dimensions $n\times n$ and $m\times m$ respectively such that $m \ge n$ and $n = p * 2 + 1 : p \in \mathbb{N}$; that is, $p$ is odd. Now let $s$ be a positive integer such that $m$ is divisible by $s + 1$. Then the convolution of $F$ over $I$ is the two-dimensional tensor created by taking the dot product of $F$ with each sub-matrix of $I$ starting at the top-left most viable position for F
(where the boundary of $F$ is flush with $I$) and taking strides of size $s$ row-wise and then column-wise and taking the dot product at each location. An example is shown in figure \ref{fig:CNN}.

\begin{figure}[h]
\includegraphics[width=\linewidth]{img/CNN.jpeg}
\caption{A layer of convolutional filters and the resulting convolution volume with an input image}
\label{fig:CNN}
\end{figure}

Fukushima proposed a convolutional architecture in \cite{fukushima1980neocognitron}, although the model did not use backpropagation (which was still not fully understood). The CNN architecture was first described in its present form by Yan LeCun in \cite{lecun1998gradient}, which used two convolutional layers followed by a deep, fully connected neural network for handwriting recognition. The architecture (and the neural network field) really took off
in 2012 with the results published by Alex Krizhevsky in \cite{krizhevsky2012imagenet}. Krizhevsky's model, dubbed AlexNet, won the ImageNet \cite{deng2009imagenet} Large Scale Visual Recognition Challenge (ILSVRC) in 2012 by a handy margin. Since then several contributions have been made to CNN design - usually in the context of scoring very high in the ILSVRC \cite{simonyan2014very,szegedy2015going, he2015deep}.

\subsubsection{Training}
Training of all neural network architectures discussed herein is done through means of backpropagation. Backpropagation is decidedly the most popular means of training neural networks. While it was toyed with in the late 60s and 70s, it wasn't until the mid 80s that backpropagation was fully appreciated by the broader machine learning community \cite{hecht1989theory}. At it's core, backpropagation is the key to stochastic gradient descent, a method of searching
the parameter space for a set of parameters that minimizes a provided \emph{loss function}. The duty of the loss function is to provide a real number that measures how the network performed on a given test set (lower is better).

Backpropagation, then, is simply computing the derivative of the loss with respect to every parameter in the network, using chain rule to "propagate" local gradients backward through the graph. In short, it simply computes the gradient of the loss. Stochastic gradient descent is the process of computing the gradient of the loss (via backpropagation), and taking a small "step" in the opposite direction (the direction of maximum decrease of the loss). It is
worth noting that backpropagation requires that the whole network be differentiable. However, linear combinations are always differentiable, so the making the graph differentiable boils down to making the activation function of the neurons differentiable.

\subsubsection{Classification}
Classification is likely the most basic task for which neural networks are used. Image classification specifically is the task of determining whether an image contains a certain class of object (e.g. a cat). State-of-the-art image classifiers are now approximately as good as humans at classifying images \cite{simonyan2014very, he2015deep}, and, regardless of how exotic they may look, they all use CNNs for feature extraction. Classification may or may
not be an important part of target tracking: once the target is acquired, classifying that target correctly and keeping track of it are not necessarily the same thing. Some of the best target-tracking software in recent years make no attempt to identify the type of target they're tracking \cite{kalal2012tracking, Nebehay2015CVPR}.

\subsubsection{Localization}
Object localization is certainly an important part of target tracking, and is a more recent use of neural networks. Localization is the task of describing where in an image an object (or objects) lie. This is done by means of drawing a bounding box around the object, and evaluated by determining the amount of overlap between the predicted bounding box, and the ground-truth box.

Obviously, localization is key to target tracking, as I not only wish to know whether the object is in the current frame, but also where it lies so that I may determine other important factors such as whether I need to turn the camera to follow it, or judge how far away it is.

Object localization in usually performed by a fully connected network or other simple classifier after image features are extracted by a CNN. However, other methods have also been developed. \cite{hong2015online} Demonstrates that back-propagating through a convolutional network can effectively yield a localization result. In fact, in that case, complex boundaries (as opposed to a simple bounding box) can be drawn around the target because object
presence is evaluated on a per-pixel basis.

\subsubsection{Recognition}
Object recognition is also important for target tracking, specifically if I want the tracker to be able to detect the target autonomously. Currently many of the best visual trackers require that the target be identified by a bounding box in the initial frame of the video.

However, for the proposed project, I do not assume that the target is necessarily in frame initially, or that a human operator is available to identify it. Rather, I would like to be able to provide a reference image to the program, and have it search for the target and find it if it appears in frame. Neural Networks have been used recently for vehicle recognition \cite{bautista2016convolutional, dong2014vehicle}, and it may be possible to use
these networks as "spotters" for target acquisition (given a vehicle type or a reference image).

Ideally, though, target acquisition is done through the same neural network that is used to track the object. Given that a CNN is already used in the model described by Hong \cite{hong2015online}, in may be possible that it would do target acquisition as well as tracking.

\section{Research Goals}

\subsection{Motivation}
Recent research on autonomous target-tracking with quad-rotors has shown promising preliminary results \cite{pestana2014computer, woods2015dynamic, boudjit2015detection, lin2012robust}, however none of these represents a completely viable solution for several reasons. With the exception of \cite{lin2012robust}, all vision logic takes place on external hardware, rather than on an on-board computer. While this is fine for applications
where the drone can be communicated with, it adds the requirement that the drone be within range of whatever communication system is in use. It is also worth noting that communication over a network, wifi or otherwise, adds latency that can vary with bandwidth. It is possible that, with enough network lag, the on-board computer would not receive control updates in time for effective real-time tracking. Also, in \cite{lin2012robust,
woods2015dynamic} it should be noted that the targets are predetermined, however I desire a vision system that can track arbitrary targets based on a reference image.

Also, to our knowledge, there does not exist an implementation of small UAV target tracking using convolutional neural networks. All implementations that I could find rely, at least in part, on hand-coded features to track the target object, and often make assumptions about how the object can move. While some of these methods show impressive results \cite{kalal2012tracking, Nebehay2015CVPR} on the benchmarks they are tested against, they lack
the flexibility of pure learning algorithms, which can switch domains simply by using different sets of training data. Further, improving on the algorithms often requires coding ever more fine-grained models of the target to be tracked and, unlike trackers based on object localization models, can track only one target at a time.

I desire an architecture that makes as few assumptions as possible and is applicable to as many domains as possible.
\subsection{Problem Definition}

I aim to make a target-tracking and acquisition system that can be run on an on-board computer on a small, hex-rotor drone. This target tracking system should be robust against object deformation, especially object magnification and partial occlusion, as I expect the system to keep track of the target even as the drone attempts to land on it.

\subsection{Goals}

I break the problem into the following goals:

\begin{enumerate}
\item Create a vision-based, monocular target-tracking program that can be run on a small, low-power computer. This computer should draw a small enough amount of power, and be of dimensions such that it could conceivably be carried and run on-board a small rotary wing drone. The program should process video frames and return a bounding box around the target if it is present, or set a flag if it is not present. The program should process frames
at a rate that allow the control system to effectively track the target, ideally at a rate $r \ge 20$ fps. It should also be robust to various common issues in target tracking: namely partial occlusion of target, object deformation and the target leaving and returning to frame. The program should not use deep-learning, but more conventional and less compute-intensive solutions like SIFT and RANSAC.
\item Create a program similar to the one mentioned above, except that its implementation should use deep-learning to track the target. The purpose here is twofold: This implementation will be used as a feasibility test for deep-learning-based tracking on low-power devices. Assuming that this implementation can run in real time, it will also be used to compare the performance of a deep-learning-based target tracking with a more
conventional, feature-based tracker. While deep neural networks have been used successfully in target-tracking algorithms in recent years, they have, to the best of my knowledge, been tested exclusively on top-of-the-line, high-power GPUs, which are entirely unfeasible for deployment on small, battery-powered vehicles like copter drones. As stated above, an added performance constraint in this research is that the program be able to run on
a low-power device that is a feasible companion computer for a battery-powered vehicle.
\item Optimize the deep-learning-based tracker for a specific, low-power architecture. Often, neural networks are designed with a specific number of cores in mind. Since most deep-learning tracking approaches are built for GPUs with a large number (1024) of cores, they're design may not be optimal for a smaller device.
\item Extend the functionality of the program described above to include target detection. Detection, as opposed to tracking, is the problem of searching for and discovering a target given a reference. In our case, the reference will be an image of the target outside of the current context (e.g., a picture of a car in a garage vs. on the street). While target tracking is important, we want the tracking vehicle to be completely
autonomous, meaning that it should have no need of human input in real time. Without the capability of target detection, the program would need human input to identify the target on first sight.
\end{enumerate}

\section{Methodology}

For our hardware, I will use the commercially available and relatively inexpensive NVidia Jetson TX1 GPGPU. In tests, the TX1 draws a maximum of 16 watts of power, a low enough energy footprint to run on a small drone. Specifically, I propose the following 

\subsection{Non-learning-based implementation} \label{goal1}
I plan to use \cite{Nebehay2015CVPR} as a non-learning based tracker. The results of this tracker are as impressive as any non-learning based tracker, and the code is openly available, so it should be easy to implement. The central idea behind \cite{Nebehay2015CVPR} is to create two object models based on "keypoints", which are small, and presumably invariant, features of the object. One model is dynamic and the other is static.
The static model is simply the first known occurrence of the object. This is presumably a trustworthy representation because it is selected by the user. The dynamic model is a set of keypoints that is updated at every time step. This model is necessary to deal with scaling, rotation, occlusion and object deformation. At each time step, optical flow is used to select a region of the image to scan for the target. Candidate keypoints are
selected from this region and then paired with keypoints from the previous time step. A clustering algorithm is used find a likely center for the object. Matching keypoints are then evaluated based on a geometric dissimilarity metric, which is the main contribution of the paper. This metric is essentially a measure for object deformation. The algorithm tolerates only a certain amount of object deformation before it decides to reject
the matching point in its representation of the object (the threshold for deformation is a hyperparameter to the algorithm, with 0 representing absolute rigidity). The most computationally intensive part of the algorithm is the clustering portion, which is if of order $O(n^2)$ complexity. However, empirically, $50 < n < 250$, so complexity never becomes a limiting factor.

\subsection{Learning-based tracker} \label{goal2}
For our learning-based target-tracker, I propose to use the network architecture described in \cite{bertinetto2016fully}. I am excited about this particular architecture, because it is fully convolutional, and doesn't require computationally expensive operations like back-propagation, or learning an online model, as in other CNN-based trackers. Rather, \cite{bertinetto2016fully} uses a pair of CNNs called, collectively, a
Siamese network. The basic concept behind Siamese networks is to use two identical networks to extract image features on both a candidate image and a reference image, and then use a simple distance metric to compare the representations of the images in feature space. The candidate image that minimizes that distance is considered to be the new target location. This approach is attractive for several reasons. First off, it is
exceedingly simple. Since the reference image never changes, we may compute it features exactly once at the beginning of tracking. From that point on, each tracking step involves simply passing candidate images through the same network, and comparing the results to the reference features. As Bertinetto shows in \cite{bertinetto2016fully}, the benefits of such a simple architecture is a vastly improved frame rate. Specifically,
Bertinetto claims to achieve frame rates of around 85 fps, a great leap forward compared to other CNN-based trackers, which appear to hover at or below 15fps. My hope in using this tracker is that, even when run on a slower machine, it will achieve real-time performance.

\subsection{Improvements to the learning-based tracker} \label{goal3}
It is possible, even likely, that a neural network designed with a 1024-core GPU in mind will not run particularly well on a single 256-core GPU. In that case, we may expect to see significant improvements to performance if the CNN architecture is changed to take into account the smaller number of cores. Furthermore, it is of note that Bertinetto uses the convolutional architecture of "Alexnet" as described in
\cite{krizhevsky2012imagenet}. A particular feature of Alexnet that may have room for improvement is the size of its filters. the Alexnet filters start at size ($1 \times 11$, then proceeds to convolutions of $5 \times 5$, and then $3 \times 3$ for the final three convolutional layers. However, there is a strong argument for using smaller filters, and making up for the spatial loss by using a deeper network. Consider the fact that
a $7 \times 7$ filter has the same receptive field (area of the image that actually affects the output neuron at a given location) as $3  \times 3$ convolutional layers. However, the smaller filters will also incorporate more nonlinearity, and decrease both the number of parameters to learn (49 vs. 27) and the number of floating-point multiplies needed for a forward pass - all of which suggest improved performance and quality.
My suggeston then, is to replace the large filters in the early stages of the Alexnet architecture with several smaller layers, and compare performance with the original implementation.
However, changing the architecture means that I can no long take advantage of "transfer learning," the practice of using weights from one learned model (e.g. Alexnet), and transferring them to a different application with the same network structure. In this case, I will train the convolutional network myself, using the same strategy as described in \cite{bertinetto2016fully}.

\subsection{Adding object recognition} \label{goal4}
While the problem of object tracking technically has nothing to do with the problem of object detection, its easy to see that, from a practical application stand point, the two are deeply related. Tracking a target is of little use if one cannot first find the target to track. Again, from a practical view, we may assume that any application that specifies autonomous target-tracking would likely also specify autonomous
target acquisition. Otherwise, a human being may have to specify the target before it could be tracked, which may be tedious or impossible in a given situation. Fortunately, because the Siamese architecture is essentially just a measure of correlation between the semantic features of two objects, we may also apply the same network to object detection. If, instead of using a region of the first image as a reference for the object, we
use a known image of the object, it becomes clear that we can simply use the same method to detect the target's presence in any frame. The main change would be in selecting a region of the image in which to look for the object. For computational efficiency, the target tracking searches a region of the image that is centered on the last known location of the object. For object detection, I propose that, if the target is not
currently in view, the algorithm switches to a detection mode wherein it searches for the object globally in each frame.

\section{Results}

\subsection{Deliverables}

\textbf{Tracking Module:} A target tracking module comprised of a Jetson TX1 embedded device with runnable target-tracking software installed. Currently, the tracking program is written to follow a target at constant speed until it is no longer in frame, however this behavior could be modified. Due to flight equipment being unavailable, the software has not yet been tested end-to-end. However the two primary modules of the software (visual target-tracker and flight controller) have individually passed functionality tests. The flight controller has shown, in simulation, that it will correctly follow a target within its view, and, as evidenced by the tracking results in \ref{trackereval} Remote connection to the TX1 is handled over wifi, which can be used to launch the tracking script and set the target.

\textbf{Application}: The tracking application, consisting of a library of visual target-trackers and a flight controller for following a target, has been made freely available via github. Documentation has been provided to assist in using the tracking module, and it is my sincere hope that someone will actually try to use it.

\subsection{Evaluation of Target trackers}\label{trackereval}

All trackers are evaluated against the Online Tracking Benchmark (OTB) developed by Yi Wu, Jongwoo Lim and Ming-Hsuan Yang in \cite{wu2013online}, and compared against each other. To the best of my knowledge, OTB is the most thorough, complete, and widely used benchmark for online tracking. Further, the authors of the SiamFC and CMT algorithms (\cite{bertinetto2016fully} and \cite{Nebehay2015CVPR}, respectively) both use OTB to evaluate their results, and using it again here provides a measure of continuity with their research.

OTB uses two metrics of prediction at each frame in a sequence, precision and intersection-over-union (IoU). In both cases, the metrics are compared against an arbitrary threshold and are considered to pass if they meet or exceed the expectations defined by the threshold, and fail otherwise.

Precision is simply the Euclidean distance between the centers of the predicted and ground-truth bounding boxes. For a given frame, the tracking algorithm is considered to have been successfully precise if that distance is $d \leq \theta_p$ where $\theta_p$ is an arbitrary threshold, usually in the range $[0, 50]$.

Precision measures how centered the bounding box is on the target, but contains no information about how well the bounding box fits the target. Precision does not penalize a predicted bounding box for being larger or smaller than the target. For this reason, OTB uses IoU and as its primary metric. IoU is defined as the area of overlap (intersection) of the predicted and ground-truth bounding boxes, divided by the total area
that they cover (union). That is, for a predicted bounding box with area $A_p$ and a ground-truth bounding box with area $A_t$, the IoU is defined by:

\begin{gather*}
\frac{A_p \cap A_t}{A_p \cup A_t}
\end{gather*}

A tracker is successful on a given frame if the IoU $\geq \theta_s$ where $\theta_s$ is an arbitrary threshold in the range $[0,1]$. For the purposes of this project I report only the IoU, and not the precision.

Each tracker receives a single IoU score for each test in the tracking benchmark (there are three, as explained later). In the simplest case, this is calculated by computing the IoU for each frame in the 100 sequences in the OTB-100 dataset, and then calculating the frequency of success at 50 evenly spaced thresholds in the range $[0,1]$. This generates a plot of success rate as a function of threshold value. The final IoU score is the area under the curve (AUC) of the IoU success rate plot.

OTB uses three separate tests, each of which generates one precision plot and one success plot. One-Pass Evaluation (OPE), Spatial Perturbation Evaluation (SPE), and Temporal Perturbation Evaluation (TPE) to evaluate the performance of online trackers.

The OPE test runs the tracker over each sequence exactly once. The tracker is given the ground-truth bounding box for the target in the first frame, and generates predicted bounding boxes for each frame thereafter. As the authors of OTB note in \cite{wu2013online}, one-pass evaluation may under- or overestimate the performance of a tracker if the initial frames and positions of the targets are, on average, particularly favorable or unfavorable for the tracker being evaluated. In order to evaluate a tracker's robustness to changes in start frame and target selection, the OTB benchmark implements the TPE and SPE tests respectively. The SPE test runs the tracker on each sequence 12 times; for each run, the initial bounding box is slightly scaled or shifted to mimic variance in user-defined bounding boxes. Similarly, the TPE test breaks each sequence into twelve sub-sequences, and runs the tracker over each subsequence, re-initializing the tracker with the first frame and ground-truth bounding box of each. As with OPE, the frame-wise metrics are computed at different thresholds, and the AUC computed.

Speed is reported in frames per second (fps).

\subsection{Environment}

All tracking algorithms were run on a Jetson TX1 running Linux for Tegra (L4T) Ubuntu 14.04 (Trusty Taur). All algorithms were implemented in python, and were modified minimally from the original code. Although the TX1 has 3.9 GiB of memory, only around 2.43 GiB memory is available in user space \textbf{[Why?]}. L4T Ubuntu is a multi-tasking OS, and is not the most efficient environment in which a tracker can run. However, it mimics the real environment in which a tracker would have to run on small drone, since the onboard computer would have to handle several tasks (e.g. communicating with ground control).

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering\captionsetup{width=.85\linewidth}%
  \includegraphics[width=\linewidth]{img/cmt_siamfc_AUC.png}
  \caption{OPE plot for the CMT and SiamFC trackers on a Jetson TX1}
  \label{fig:ope_vs}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering\captionsetup{width=.85\linewidth}
  \includegraphics[width=\linewidth]{img/all_trackers_AUC.png}
  \caption{OPE plot for several trackers, SiamFC is the best performing.}
  \label{fig:ope_all}
\end{subfigure}
\caption{One-Pass Evaluation (OPE) results for the CMT and SiamFC trackers}
\label{fig:ope}
\end{figure}

\subsection{Tracking Results}\label{tracking_results}

Figure \ref{fig:ope} shows the results of the OPE test. As is evident from plot \ref{fig:ope_vs}, the SiamFC algorithm far outperforms the CMT algorithm; and plot \ref{fig:ope_all} shows that SiamFC outperforms several other well-known feature-based tracking algorithms. However, the SiamFC algorithm ran at an average of 2.21 fps over the OPE test, while the CMT algorithm ran at an average of 10.70 fps [\textbf{verify this number}]. Notably, the SiamFC tracker did not have enough memory to store the entire computational graph in memory, which likely adversely affected its speed.

\section{Conclusion and Future Work}

\subsection{Conclusion}
In this thesis I have implemented and demonstrated the feasibility of a tracking module for small, low-power devices. The CMT tracker coupled with sound control logic, could conceivably be used to autonomously track an object. I have also shown that a learning-based algorithm, such as the SiamFC algorithm, can be run on a low-power computer such as the Nvidia Jetson TX1. Although its performance was too slow to currently be considered for online tracking, it is not inconceivable that, with some optimization, it could reach real-time speed. This would allow small autonomous devices to take advantage of the powerful computer vision applications of neural networks.

\subsection{Future Work}
Several questions are left unanswered by this research. Foremost is the question of whether a neural network large enough to perform at or near state-of the art tracking performance (i.e. on the OTB benchmark) can be run in real time on a device as small as the Jetson TX1. Given that lack of sufficient memory for the network is the most likely culprit for the slow speed of the SiamFC algorithm on the TX1, reducing the memory load of the algorithm is an obvious first step.

A couple of options exist for reducing memory load. The first is to change the network architecture. The authors of the SiamFC algorithm used convolutional layers inspired by Alexnet in \cite{krizhevsky2012imagenet}. However, as shown in \cite{simonyan2014very}, a deeper network architecture with smaller convolutional kernels can simultaneously decrease the number of parameters in the network, and increase performance. Perhaps using an updated architecture could both reduce memory load without decreasing (and possibly increasing) tracking performance.

A second approach would be to keep the same basic architecture (i.e. kernel size and depth) while reducing the total number of kernels in each layer.

Because these changes occur along different axes (kernel-size and depth, and kernel count), they can be combined. For example, we may use the first 10 convolutional layers of VGGNet while reducing the number of kernels in each layer by half.

Another obvious solution to the memory problem would be to simply increase the available memory on the device. As a research goal this is not of much interest, but it would serve to validate whether having the whole network in memory significantly increases performance and, if so, by how much.

There is also ample room for further research in the domain of control logic. The resulting module of this research uses a fixed camera that is mounted on the logical "front" of the copter drone, which limits its utility. Ideally, the module would use a camera mounted on a gimbal and implement two control logics: one for the camera, and one for the drone itself. An ideal control situation would be one that keeps the camera tracking the object of interest, while drone itself is free to navigate an unrelated pattern (e.g. circling the target, or executing maneuvers for collision avoidance).

\newpage
\bibliography{references}
\bibliographystyle{ieeetr}
\end{document}
}})
