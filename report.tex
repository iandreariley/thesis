\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{ {figures/} }
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[left=1.5in, top=1in, right=1in, bottom=1in]{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{makecell, multirow}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{algorithmicx}

\usepackage{fontspec}
\setmainfont{Times New Roman}

% Load Fonts
% \setromanfont[
% BoldFont=timesb.ttf,
% ItalicFont=timesi.ttf,
% BoldItalicFont=timesbi.ttf,
% ]{times.ttf}
% \setsansfont[
% BoldFont=arialbd.ttf,
% ItalicFont=ariali.ttf,
% BoldItalicFont=arialbi.ttf
% ]{arial.ttf}
% \setmonofont[Scale=0.90,
% BoldFont=courierb.ttf,
% ItalicFont=courieri.ttf,
% BoldItalicFont=courierbi.ttf,
% Color={0019D4}
% ]{courier.ttf}

\doublespacing
\newgeometry{left=1.5in, top=2in, right=1in, bottom=1in}
\title{Target-Tracking Using Neural Networks on Low-Power Devices with Applications to UAVs}
\author{Ian Riley}
\date{December 2016}

\begin{document}
\pagenumbering{roman}

\begin{titlepage}

	\thispagestyle{empty}
	\begin{center}
		\textbf{Target-Tracking Using Neural Networks on Low-Power Devices with Applications to UAVs}\\
		\vspace{60pt}
		A Thesis\\
		Presented to the\\
		Faculty of\\
		California State Polytechnic University, Pomona\\
		\vfill
		In Partial Fulfillment\\
		Of the Requirements for the Degree\\
		Master of Science\\
		In\\
		Computer Science\\
		\vspace{60pt}
		By\\
		Ian Riley\\
		2017
	\end{center}
\end{titlepage}

\newpage
\begin{center}
\section*{Signature Page}
\addcontentsline{toc}{section}{\numberline{}Signature Page}%
\end{center}
\addtocounter{page}{1}
{
	\setstretch{1.0}
	\begin{tabularx}{\textwidth}{lX}
		& \\
		& \\
		\textbf{\uppercase{Thesis:}} & \uppercase{Target-Tracking Using Neural Networks on Low-Power Devices with Applications to UAVs} \\
		& \\
		& \\
		\textbf{\uppercase{Author:}} & Ian Drea Riley \\
		& \\
		& \\
		\textbf{\uppercase{Term Submitted:}} & Fall 2017 \\
		& \\
		& Computer Science Department \\
		& \\
		& \\
		Dr. Amar Raheja & \hrulefill \\
		Thesis Committee Chair & \\
		Department of Computer Science & \\
		& \\
		& \\
		Dr. Subodh Bhandari & \hrulefill \\
		Department of Aerospace Engineering & \\
		& \\
		& \\
		Dr. Hao Ji & \hrulefill \\
		Department of Computer Science & \\
	\end{tabularx}
}

\newpage
\newgeometry{left=1.5in, right=1in, top=1in, bottom=1in}
\section*{Acknowledgements}
\addcontentsline{toc}{section}{\numberline{}Acknowledgements}%
Completing this thesis would have been impossible were it not for the tremendous good fortune I've had to be surrounded by loving, intelligent, and supportive people. To Mom and Dad, first of all, for supporting your decreasingly youthful son while he played with computers. To Alexandra, for listening to me talk about neural networks with something that passed for interest; you are first and last in my heart. To Devin, for the soundest career advice; wubba-wubba. To my thesis committee:
Dr. Amar Raheja, for your ongoing academic and professional support, I owe you a debt of gratitude too large to repay here; Dr. Subodh Bhandari, for teaching me to google; Dr. Hao Ji for a most thorough introduction to neural networks. To Arvin, Joshua, Alex, Fernando, Miguel, and Rene; a finer group of aerospace engineers was never assembled. To Luca Bertinetto and Jack Valmadre, for publishing a fantastically elegant tracking algorithm, and then answering my thousand questions about how it works. To Jim at jetsonhacks.com, for rad technical support
and radder jam sessions. To my friends, for giving me the thumbs up even when they probably shouldn't have.
\newpage
\section*{Abstract}
\addcontentsline{toc}{section}{\numberline{}Abstract}%
Recent advances in computer vision and general purpose graphic processing units (GPUs) have led to significant advances in computer-vision-based object detection and tracking. Simultaneously, research on low-power computing has yielded battery-powered devices with enough compute power and memory to run resource-hungry algorithms like neural networks. At the same time, small unmanned aerial vehicles are drawing interest for a wide array of applications, like surveillance. Despite this confluence of events, there seems to be a dearth of research for onboard tracking for small UAVs.
In this research I implement a small onboard tracking module intended for a hexacopter UAV. I implement two computer-vision-based tracking algorithms for the module and compare their performance. One is a so-called "feature-based" tracker, the other uses a convolutional neural network. The results show that the neural-network-based method out-performs the feature-based method, but runs too slowly for a real-time application.
%Automated target-tracking has long been a problem of interest in the field of computer vision. This is motivated by the fact that visual surveillance is a simple and tedious task. Yet truly robust solutions for visual target tracking have been illusive. The problem at the center of persistent target tracking, is how to make the tracking program robust to changes in the target's appearance. An ideal tracker would be robust to changes in object scale and rotation, changes in lighting,
%object deformation and full- or partial-occlusion. Recent advances in object classification using convolutional neural networks (CNNs) have made them popular for use in anything computer-vision related. Until recently a lack of sufficient labeled video data limited attempts at using CNNs for target tracking to hacking the features of trained object classifiers for spatial data, operations that are too expensive for the real-time requirements of target trackers. However,
%\cite{bertinetto2016fully} uses the recently released ImageNet video dataset to train a fully-convolutional network that is capable of tracking with state-of the art accuracy, at more than 10 times the speed of some CNN-based trackers.
%At the same time, computer hardware has become cheaper, faster, and increasingly focused on parallel processing. Because neural-network architecture is meant to mimic the highly parallel architecture of the brain, they pair naturally with massively parallel computer architectures like those of GPUs, which often have hundreds, if not thousands, of small, low-speed cores. The advance of GPU architecture has made running neural-networks at real-time speeds achievable. More recent
%GPUs have been designed specifically with low-power devices in mind. While they do not deliver the computational power of larger, high-power GPUs, they make highly-parallel processing available to battery-operated devices.
%In this research, we attempt to show that a real-time target-tracking software can be run on a small, low-power GPU that could feasibly be installed as a companion computer to a small robot, and give it target-tracking capabilities.

\newpage
\tableofcontents

\newpage
\listoffigures

\newpage
\pagenumbering{arabic}

\section{Introduction}

Over the past decade, small unmanned aerial vehicles (UAVs) have drawn increasing interest for their potential range of utility in civilian, industrial and military applications. Rotary wing UAVs have received special attention. Their ability to take-off and land vertically, hover and reduce airspeed arbitrarily without stalling makes them particularly well suited to navigate several otherwise difficult but interesting environments such as indoor environments, cities, and any outdoor environment that has constrained spaces or a preponderance of physical obstacles.

During the same time period, there has been an increased interest in autonomous robotics. Improved artificial intelligence (AI) algorithms have made the possibility of robots autonomously carrying out helpful tasks a viable one; a fact which has not gone unnoticed by commercial and defense industries. Not surprisingly, there have been several attempts to make UAVs capable of performing various flight missions autonomously; these include flight control \cite{zhang2015autonomous}, cargo delivery \cite{zhao2015robust}, and target tracking \cite{lin2012robust,boudjit2015detection,woods2015dynamic,pestana2014computer}.

Target tracking is of particular interest because it generalizes other more specific problems like runway approach, and cargo pickup / delivery. It also has obvious applications in such disparate areas as surveillance, wildlife observation, and sports cinematography.

Target-tracking by itself has proven to be a difficult computer vision task, and is made even more difficult when constrained by real-time performance, as is necessary for autonomous target-tracking. Until recently, the target-tracking problem has been dominated by feature-based algorithms that build models of the object to be tracked \cite{kalal2012tracking,Nebehay2015CVPR}. These models, however, are often not robust to severe changes in the appearance of the target, and their models tend to drift. The advent of convolutional neural networks (CNNs) for computer vision applications in recent years has given rise to several new, deep-learning-based approaches to the target tracking problem. Within the last 5 years, convolutional neural networks (CNNs) have produced outstanding results in the field of image recognition, both in classification and object localization \cite{krizhevsky2012imagenet,simonyan2014very,szegedy2015going,he2015deep}. CNNs have been shown to successfully extract high-level features directly from raw data, which avoids the potential for logical errors in hand-coded features.
Not surprisingly, CNNs have also been employed in the domain of object identification and tracking in video with some success\cite{redmon2015you,ning2016spatially,he2015deep,li2016deeptrack,wang2013learning,wang2015visual,ma2015hierarchical,nam2015learning,bertinetto2016fully}. Object detection and tracking in video poses problems additional to those posed by simple object classification and localization - notably to "remember" a particular target when it is occluded for a span of time (e.g. a car going under an overpass) or if it momentarily leaves the camera frame, or when there are several objects in view that are similar to the target. Although there is some recent evidence that the use of recurrent neural networks (RNNs) in tandem with the features extracted by CNNs may make CNN based trackers more robust to gaps in target visibility \cite{ning2016spatially}.

Given the clear utility of a drone capable of visually tracking a target, little research appears to have been done towards a viable, robust, and onboard solution to the target tracking problem, and what research there is makes no use of deep-learning-based algorithms. Further, several of the algorithms \cite{pestana2014computer, woods2015dynamic} do all computer-vision related tasks on a remote computer (i.e., not onboard) which places a significant constraint on the UAV, since it must always maintain a line of communication with the remote.

In this thesis, I present two implementations of an onboard target-tracking module, one using a feature-based detection algorithm, and another using a CNN-based detection algorithm, and compare their results. I find that while the CNN-based tracker outperforms the feature-based tracker by a wide margin, its frame-rate is too slow to meet real-time requirements.

%Target-tracking using CNNs, then, seems like a promising route for developing the tracking capacity necessary for autonomous drone surveillance. However, the high computational cost of CNNs as compared to other tracking methods poses a challenge in the realm of real-time target tracking. General Purpose Graphic Processing Units (GPGPUs) are used to reduce the compute-time required for prediction using CNNs, however only a very few architectures have managed the speed required for real-time tracking.

%A further complication is that small drones operate on very small power supplies. The fastest GPGPUs, which are often the test hardware for the previously mentioned target-tracking architectures, have TDPs of around 250 watts - far too much power for a quad-copter to deliver on its limited power supply. Recent GPGPU designs geared specifically for low-power applications have made it possible to run complex parallel applications like CNNs in low-power environments, like a small battery-powered drone. Although they do not provide as much computational power as their larger, more power-hungry counterparts, it is our hope that I may successfully run neural-network-based target tracking software on these modules, in flight. 

\newpage
\section{Literature Review}

\subsection{UAV target tracking}

Research on monocular, vision-based target tracking for UAVs appears sparse, despite continuing interest and advances in vision-based target tracking. For small, rotary wing drones I know of only a few previous attempts to automate target tracking \cite{woods2015dynamic,lin2012robust,boudjit2015detection,pestana2014computer}.

In \cite{lin2012robust}, which is the work most closely related to this research, Feng Lin uses an onboard CPU on a small unmanned helicopter to do all computer vision processing for the purpose of target tracking. Object detection is accomplished through a multistep process of color thresholding, morphological augmentation, and Kalman filtering. The image is converted from HSV format to binary format by thresholding the HSV image against a preset range of colors. All pixels that do not fall in the range are assigned a zero in the binary image; all other pixels are assigned a one. Next, the pixels of interest in the binary image (ones) are grouped into regions of interest (ROIs). Kalman filtering is then used to add dynamic features to the object. A bayesian discriminant filter is applied to the features of each ROI to pick one as the new target position.

However, for this tracking algorithm to work, it makes several strong assumptions about the target. First, it assumes that the target is \emph{Lambertian Reflective}, that is, that it has an ideally \emph{matte} surface, meaning its brightness doesn't change with the angle it's viewed from. Second, it assumes that the color distribution of the object is distinct from the color distribution of its surroundings. Third, and perhaps most restrictive, the target must be chosen from a predefined set of targets. While this is a good step in the direction of tracking autonomy, it is clearly too limited for general-purpose tracking. The first two constraints suggest the tracker could be easily fooled by reflective, luminous, and camouflaged objects, and the restriction of targets to a predefined list means that a database would have to be updated before tracking could begin. However, humans have no such restrictions on their ability to follow objects visually. In order for target tracking to be truly autonomous, it must be as good as or better than human performance.

Jesus Pestana uses a much more robust tracker in his work in \cite{pestana2014computer}. The tracker, dubbed TLD \cite{kalal2012tracking}, uses both features and deep learning for object detection, and is discussed further in section \ref{feat_tracking}. TLD is not subject to the constraints in Lin's algorithm, which makes the results of Pestana's work much more compelling. However, Pestana's work relies on a remote computer to do all image processing, which restricts the use of the drone to areas where it can maintain constant communication with the remote.

The work in \cite{woods2015dynamic,boudjit2015detection} both use remote computers for image processing as well and, in the case of \cite{woods2015dynamic} an external sensor system as well. These works certainly do important work to demonstrate the feasibility of computer-vision-based control logic, however, they cannot be considered for broad practical applications because they are tethered to external equipment.

Among these works, only \cite{lin2012robust} uses an onboard target tracker; the rest handle tracking and control logic on a remote computer that the drone communicates with via wifi. None of the cited works use neural-network-based trackers. To the best of my knowledge, there is no existing work that attempts to use deep-learning-based computer vision for onboard target tracking. This is likely due to the absence, until very recently, of computer architectures that were simultaneously, small, low-power, massively parallel and fast. Small copter drones can only carry very small payloads. The hexacopter drone used for this research required that the payload be less than 5 oz. Recent advances in computer hardware, notably the availability of low-power GPUs, allow for a computer to be carried on-board a small drone. Further, these small, highly parallel modules do not draw much power, so they do not require drones to carry larger batteries, and can compute large amounts of data in parallel, which means that neural networks can run quickly on them.

One of the major difficulties posed by monocular target tracking in the domain of drone control is range estimation. Since the drone itself is presumed to be moving, and has real physical boundaries, it is important that the drone keep a safe distance from the target, or approach it at a reasonable speed. Because a pinhole camera provides a single 2D projection of a 3D space, range cannot be calculated directly, but must be estimated from the
image. However \cite{dobrokhodov2006vision} shows that, if the target can be assumed to be a land vehicle, range estimation is not particularly difficult; although it is worth noting, that estimation noise is less tolerable as the UAV approaches the object, as in the case of making a landing.

\subsection{Object Tracking}

In the broadest terms, object tracking can be defined as follows: Given a sequence of images and the target object's position in the first image, return the target position for all subsequent images. For the purposes of this paper, we use the definition implied by the Online Tracking Benchmark \cite{wu2013online}: The position of a target is given by a polygonal region in the first image of a sequence, and the tracking function must return a bounding box (i.e. a rectangle normal to the axes of the image) for all subsequent frames that tightly matches the bounding box drawn by a human being.

A wide variety of approaches have been taken to the tracking problem, most of them depending on a model of the target built during training. More recently, deep-learning algorithms have been successfully incorporated into tracking algorithms to make the models more robust to holes in image-processing logic.

In even the most mundane circumstances, trackers must overcome a wide variety of obstacles in order to perform well. Indeed, \cite{wu2013online} defines eleven different "attributes" of video sequences that can impede tracking performance. Among these are occlusion, rotation both in and out of the image plane, and background clutter. To accomplish this, the tracker must often build a model of the target that incorporates several views of the target. However, a dynamic model introduces the possibility that the model can drift, meaning it starts to incorporate background information until it loses the target completely. In this sense, the target tracking problem is largely one of creating an algorithm that is robust to changes in target appearance while eliminating drift.

\subsubsection{Feature-based methods}\label{feat_tracking}

So-called "feature-based" methods of object tracking rely on image features computed by various image processing techniques (e.g. edges, histograms, optical flow) to generate models of the targeted object and of the background. Using these models, logic is implemented to find the object in each frame. Two major non-NN approaches have been taken to visual object-tracking using hand-crafted representations: Generative and Discriminative. Generative models use past instances of the object to find it in the current frame, whereas discriminative methods attempt to distinguish the target from the background by building a representations of the target and background. How these representations are built is the major issue at hand in feature-based detection and tracking: It must be robust enough that the object is not lost in the background - but it also must be flexible enough to tolerate object deformation, and changes in view.

Perhaps the most mature feature-based tracking architecture is the well-known Tracking Learning Detection (TLD) algorithm introduced by Kalal in \cite{kalal2012tracking}. In the taxonomy of trackers I use in this report (either learning-based or feature-based) TLD is best described as a hybrid method. It relies in part on image processing techniques and features to generate bounding boxes, but it also makes heavy use of machine learning. As the name implies, TLD decomposes the problem of robust, long-term object tracking into three distinct problems:
\begin{enumerate}
\item \emph{Tracking}: The problem of finding an object in a new image frame given its location in a previous frame.
\item \emph{Detection}: The problem of finding an object in an image given a model of that object.
\item \emph{Learning}: The problem of updating the detection algorithm to incorporate new views of the object being tracked.
\end{enumerate}
At first, the difference between tracking and detection may seem trivial. However, as Kalal points out, the two tasks are distinct because detection involves localizing an object within an image based on a object model, whereas tracking uses information about target location in previous frames to determine the target location in the current frame. Accordingly, tracking and detection are prone to different types of errors: trackers tend to drift from their targets, and almost certainly fail when their target leaves frame and returns. Detectors, on the other hand, will always correctly detect the object assuming that it matches the object model with a certain degree of confidence. However, because object models are generally static, detectors fail when the appearance of the object changes via occlusion, rotation, deformation, etc. The key insight of TLD is to iteratively train the detection algorithm using a training set that is constantly updated with previously misclassified (false positive or false negative) image patches. Kalal uses a pair of expert systems, one for finding false positives and one for finding false negatives, to add new image patches to the training set at each time step. The detector is implemented as an ensemble of Bayesian classifiers and a nearest neighbor classifier that is retrained on the training set at regular intervals.

For the tracking component, Kalal uses the Median-Flow tracker he developed in \cite{kalal2010forward}. The Median-Flow tracker uses the concept of optical flow \cite{horn1981determining} to determine the frame-to-frame movement of keypoints on the object. The tracker uses optical flow to predict the new position of keypoints in each frame, and then eliminates some of them based on a confidence metric. Since the tracker is prone to drift and error, the detector is used to reinitialize the tracker if it drifts to far from the target.
 
A more recent example of a high-performing, feature-based model is in \cite{Nebehay2015CVPR}. The central idea behind \cite{Nebehay2015CVPR} is to use the first frame to find a set of object "keypoints", which are small, and presumably invariant, features of the object, and then track the movement of those keypoints through the remaining frames. To do this, the algorithm uses two models of the object. One of the models is static while the other is "adaptive" meaning it is updated with new information about the object during tracking. The static model is simply the set of keypoints discovered in the first frame. This is a trustworthy representation of the object because it is selected by the user, and will not drift, since it is never updated. The dynamic model is the set of target keypoints in the previous frame. This model is necessary to deal with scaling, rotation, occlusion and object deformation.

At each time step, the following procedure is used to find the new target location: The entire image is searched globally for keypoints that correspond to the keypoints in static model. A point in the current frame is considered a match to a point in the static model, if the distance between them is below a certain threshold, and they are much closer to each other than any of the other candidate keypoints. Matches to the adaptive model are found using optical flow. If both the static and adaptive models match the same keypoint, the static match overrules, as the static model is invariant throughout tracking. Candidate keypoints are then grouped using an agglomerative clustering algorithm along a dissimilarity metric given as
\begin{gather*}
D(m_i, m_j) = \lVert (x_i^t - Hx_i^0) - (x_j^t - Hx_j^0) \lVert
\end{gather*}
where $ m_k $ is a match $ (x_k^0, x_k^t) $ between a keypoint candidate in the current frame and a keypoint from the static model, and $H$ is the estimated linear transformation of the rigid object from the initial frame to the current frame. $D$ is a metric of object deformation. Two matches, $m_i$ and $m_j$ are seen as sufficiently similar if $D(m_i, m_j) > \delta$, where $\delta$ is an arbitrary threshold. Note that $\delta=0$ requires that the object be entirely rigid. Matching keypoints are then evaluated based on a geometric dissimilarity metric, which is the main contribution of the paper. The largest cluster of matches is taken to be the object. Other candidate points are then added to the object by matching candidate points using only those keypoints from the static model that have a potential match that could be added to the object cluster under $D$ with threshold $\delta$. This is done to increase the quantity of matching points based on the new approximation of the object center, while reducing the addition of erroneous points. 

The scale of the model is then estimated using the following heuristic
\begin{gather*}
s = median \Bigg( \frac{ \lVert x_i^t - x_j^t \lVert }{ \lVert x_i^0 - x_j^0 \lVert }, i \neq j \Bigg)
\end{gather*}
which is, for all pairs of matching points, the media ratio of the distance between the points in the initial frame, and in the current frame. Intuitively, this should be robust to local deformations in the object (even small ones), because the median is not affected by outliers. However, it should also capture deformations that affect the overall size of the object, because the ratio will change for all points.

The most computationally intensive part of the algorithm is the clustering portion, which is if of order $O(n^2)$ complexity. However, empirically, $50 < n < 250$, so complexity never becomes a limiting factor.

\subsubsection{Neural-Network methods}

Several recent publications show that CNNs can make state-of-the-art target trackers. How the CNNs are used varies widely. Some extract features from pretrained CNN classifiers \cite{hong2015online}, while others use CNNs for object correlation \cite{bertinetto2016fully}. Perhaps the simplest approach is described in \cite{redmon2015you}. Tracking in this case is reduced to object classification and localization performed at real-time speed. It's worth noting that this approach is not a fully fledged target tracker, in that the CNN generates a bounding box for every object that surpasses a given threshold on any of the object classes. There is no way to pick a specific target to the exclusion of others. However, the work offers an important insight: that using a regression strategy for localizing objects is far more efficient than using region proposal networks, and performs almost as well. This results in a CNN that can operate at real-time speed with very good performance, while other CNN models for localization are far too slow (on even the fastest hardware) to perform in real-time. The work in \cite{ning2016spatially} improves on \cite{redmon2015you} by using a recurrent neural network (RNN) to track the movement of objects through time. This is an intriguing direction because, unlike the following methods, the RNN is trained to recognize sequential image patterns (that is, the possible transformations an object can take given a current position).

Prior to the release of the 2015 ImageNet object-tracking training set, there was no tracking-specific dataset large enough to train a deep CNN. However, several algorithms found ways to make use of CNNs' noted ability to extract features from images by training them on larger classification datasets. \cite{wang2013learning} uses a deep auto-encoder to learn image features offline. Targets are then learned on-line using the features extracted from images by the auto-encoder.  \cite{hong2015online} uses the image features from a pretrained CNN to train a simple classifier (SVM) on-line to create a model of the target. Interestingly, Hong uses "deconvolution" to create a heat-map out of the input frame. The SVM then uses the heat-map to learn to find the target. So-called deconvolution is the process of back-tracking through the convolutions to find out which input data (pixels) most "excited" the neuron responsible for identifying the target. The work in \cite{wang2015visual} and \cite{ma2015hierarchical} actually seeks to discover the utility of each layer of a pre-trained CNN, and uses that knowledge to make a more informed model of the target. They show that certain convolutional layers encode views of the target from different perspectives, while other layers encode meaning like categorization. From these layers they develop a model of the target, and use it to track the target with high accuracy. A more complex approached, used in \cite{nam2015learning}, is to train domain-specific layers to the CNN that handle specific challenges (e.g. object rotation, object occlusion) in target-tracking. While complicated, the results appear more robust across various challenges.

Recently, the availability of the large ImageNet 2015 object-tracking video dataset has allowed for proper training of deep-learning based target-trackers. These works focus on using a large video dataset to train CNNs for object correlation, and then using the resulting network to search patches of each image frame for the object. \cite{li2016deeptrack} trains an ensemble of CNNs on-line, starting with an initial known instance of the target, to differentiate the target from the background. In \cite{nam2016learning}, Nam et al. learn shared convolutional layers over the entire dataset, and then implement "domain-specific" fully-connected layers where a "domain" is a training sequence. During tracking, a final layer is learned online from the output of the domain networks. Further, the domain specific layers are fine-tuned online. The resulting algorithm, dubbed MDNet (multi-domain network), achieves outstanding results, although its frame-rate is around 1 fps on top-of-the-line hardware, making it unsuitable for realtime tracking. The SANet algorithm described in \cite{fan2016sanet} improves upon these results somewhat by using a recurrent neural network (RNN) to encode information about the object's structure. This is largely to avoid errors in tracking introduced by "distractors" near the target, i.e. similar looking objects that are near or overlap with the target to be tracked (e.g. when tracking a person in a crowd, it is easy for the tracker to drift to the wrong individual). The frame-rate, however, is again too low to be considered for realtime tracking.

The work of Bertinetto and Valmadre in \cite{bertinetto2016fully}, which I use for the implementation of a deep-learning-based tracking algorithm, shows that, with this large amount of training data, a simple "Siamese" network can be trained to successfully track an object, without resorting to computationally expensive methods like back propagation or online learning. Despite the simplicity of the algorithm in \cite{bertinetto2016fully} (dubbed SiamFC by its authors for "siamese fully convolutional"), the resulting target tracker achieves near state-of-the-art performance at a frame-rate of 85 fps, which is more than adequate for realtime applications.

The Siamese network in \cite{bertinetto2016fully} is actually a single network (or single set of weights) trained to correlate two images. The network is fully convolutional (meaning it has no fully-connected layers, and no "padding" in the traditional sense), and uses an architecture similar to the convolutional layers of Alexnet in \cite{krizhevsky2012imagenet}. First, feature vectors of each image are extracted by passing them through the network. Then the two feature vectors are compared to each other with some sort of similarity metric (in this case, the dot product of the two vectors is used). During training, the network is taught to maximize the similarity score between image patches containing the same object, and minimize it between those that do not. During the inference stage, if the outputs are considered similar enough, the network predicts that they are the same object, otherwise, it predicts that they are different objects.

One of the benefits of the fully-convolutional architecture of the Siamese network in \cite{bertinetto2016fully} is that it can take image patches of any size greater than or equal to the receptive field of the final layer of the network (in this case, the receptive field is $127 \times 127$). Bertinetto and Valmadre use this to their advantage by using a larger "search" image patch at each frame that generates a map of features at adjacent locations in the image. The feature vector of the object (which is taken from the only known occurrence of the object in the first frame) is then correlated in discrete steps over each of these locations to generate a "score" map, where the top score is taken to be the true location of the object. This makes for extremely fast and precise localization. A diagram of the architecture is shown in figure \ref{fig:siamese}.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/siamesefc_conv-explicit.jpg}
	\caption{Siamese network as described in \cite{bertinetto2016fully}. In this case $\varphi$ is the CNN function. The dot product of extracted features of the template and the search image are taken at every location in the search image. The smallest dot 			      product is considered the most likely location of the object. Figure credit: \cite{bertinetto2016fully}}
	\label{fig:siamese}
\end{figure}

Rather than perform an exhaustive search for the object at each time step, SiamFC generates a set of image patches at 5 different scales centered on the previous object location and generates a score map for each using the Siamese network. A cosine window is then added to the scores to penalize translation from the center of the previous location, and score maps are penalized by the degree of scaling. Finally the maximum score from all score maps is chosen as the new location of the object.

\subsection{Convolutional Neural Networks and Computer Vision}
As noted above, CNNs have, in recent years, proved adept at image classification. The CNN architecture is a special case of the more general "feed-forward" architecture.
\subsubsection{Feed-Forward Neural Networks}
Feed-forward neural networks can be represented as an acyclic computational graph composed of "layers" of vertices, where each vertex is connected to any number of vertices in the layers adjacent to it, but not connected to any of the vertices in its own layer. Each edge in the graph has an associated weight, and the output of each vertex is the sum of the weighted outputs of the previous layer's vertices passed through some nonlinear function.

Perhaps the simplest of these architectures is a "fully connected" neural network, in which each neuron (vertex) is connected to all neurons in the prior and subsequent layers. A small instance is shown in figure  \ref{fig:fcnn}. Indeed, fully connected neural networks have a wonderfully simple architecture when compared with CNNs and, for a similar number of parameters (edges), compute a forward pass significantly faster. However, the number of parameters in a
fully connected neural networks grows very quickly with the size of the input ($O(n)^2$). For large and deep networks, the number of parameters becomes too large to keep in memory, and computation slows drastically. The CNN architecture reduces the total number of parameters by "sharing" them across neurons in a given layer. It turns out this method also preserves the spatial locality of image features, which is of obvious value in object localization and
classification.

\begin{figure}[h]
\includegraphics[width=\linewidth]{img/FCNN.png}
\caption{A fully connected neural network. Figure credit \cite{nndiagram2005burgmer}}
\label{fig:fcnn}
\end{figure}

\subsubsection{Convolutional Neural Network Architecture}
The architecture of fully connected networks makes the naive assumption that any number of inputs may have a relationship with each other; that is, that, combined, they define a feature. In the image domain, where each element of the input vector is a pixel, this is almost never the case. It is far more likely that two adjacent pixels are related to each other (for example, are part of the same object) than it is that pixels on opposite edges of the image are. CNNs make use of this spatial relationship by grouping weights together into small, square \emph{filters} (also called \emph{kernels}) that are passed over the image in discrete steps. Each step represents a neuron in the next layer. So the weights in each filter are shared across neurons. This drastically reduces the number of parameters necessary for a network with large input, and uses the parameters more efficiently. This is especially true because the early layers of a deep neural network end up tasked with extracting low-level features - like straight lines and areas of intense color. These features are common throughout images, so it makes sense to use a single set of weights to look for them rather than an individual set for each area in the image.

More formally, let $F$ and $I$ be two two-dimensional tensors of dimensions $n\times n$ and $m\times m$ respectively such that $m \ge n$ and $n = p * 2 + 1 : p \in \mathbb{N}$; that is, $p$ is odd. Now let $s$ be a positive integer such that $m$ is divisible by $s + 1$. Then the convolution of $F$ over $I$ is the two-dimensional tensor created by taking the dot product of $F$ with each sub-matrix of $I$ starting at the top-left most viable position for F
(where the boundary of $F$ is flush with $I$) and taking strides of size $s$ row-wise and then column-wise and taking the dot product at each location. An example is shown in figure \ref{fig:CNN}.

\begin{figure}[h]
\includegraphics[width=\linewidth]{img/CNN.jpeg}
\caption{A layer of convolutional filters and the resulting convolution volume with an input image. Figure credit \cite{convnetdiagram2016karpathy}}
\label{fig:CNN}
\end{figure}

Fukushima proposed a convolutional architecture in \cite{fukushima1980neocognitron}, although the model did not use backpropagation (which was still not fully understood). The CNN architecture was first described in its present form by Yan LeCun in \cite{lecun1998gradient}, which used two convolutional layers followed by a deep, fully connected neural network for handwriting recognition. The architecture (and the neural network field) really took off
in 2012 with the results published by Alex Krizhevsky in \cite{krizhevsky2012imagenet}. Krizhevsky's model, dubbed AlexNet, won the ImageNet \cite{deng2009imagenet} Large Scale Visual Recognition Challenge (ILSVRC) in 2012 by a handy margin. Since then several contributions have been made to CNN design - usually in the context of scoring very high in the ILSVRC \cite{simonyan2014very,szegedy2015going, he2015deep}.

\subsubsection{Training}
Training of all neural network architectures discussed herein is done through means of backpropagation. Backpropagation is decidedly the most popular means of training neural networks. While it was toyed with in the late 60s and 70s, it wasn't until the mid 80s that backpropagation was fully appreciated by the broader machine learning community \cite{hecht1989theory}. At it's core, backpropagation is the key to stochastic gradient descent, a method of searching
the parameter space for a set of parameters that minimizes a provided \emph{loss function}. The duty of the loss function is to provide a real number that measures how the network performed on a given test set (lower is better).

Backpropagation, then, is simply computing the derivative of the loss with respect to every parameter in the network, using chain rule to "propagate" local gradients backward through the graph. In short, it simply computes the gradient of the loss. Stochastic gradient descent is the process of computing the gradient of the loss (via backpropagation), and taking a small "step" in the opposite direction (the direction of maximum decrease of the loss). It is
worth noting that backpropagation requires that the whole network be differentiable. However, linear combinations are always differentiable, so the making the graph differentiable boils down to making the activation function of the neurons differentiable.

\subsubsection{Classification}
Classification is likely the most basic task for which neural networks are used. Image classification specifically is the task of determining whether an image contains a certain class of object (e.g. a cat). State-of-the-art image classifiers are now approximately as good as humans at classifying images \cite{simonyan2014very, he2015deep}, and, regardless of how exotic they may look, they all use CNNs for feature extraction. Classification may or may
not be an important part of target tracking: once the target is acquired, classifying that target correctly and keeping track of it are not necessarily the same thing. Some of the best target-tracking software in recent years make no attempt to identify the type of target they're tracking \cite{kalal2012tracking, Nebehay2015CVPR}.

\subsubsection{Localization}
Object localization is certainly an important part of target tracking, and is a more recent use of neural networks. Localization is the task of describing where in an image an object (or objects) lie. This is done by means of drawing a bounding box around the object, and evaluated by determining the amount of overlap between the predicted bounding box, and the ground-truth box.

Obviously, localization is key to target tracking, as I not only wish to know whether the object is in the current frame, but also where it lies so that I may determine other important factors such as whether I need to turn the camera to follow it, or judge how far away it is.

Object localization in usually performed by a fully connected network or other simple classifier after image features are extracted by a CNN. However, other methods have also been developed. \cite{hong2015online} Demonstrates that back-propagating through a convolutional network can effectively yield a localization result. In fact, in that case, complex boundaries (as opposed to a simple bounding box) can be drawn around the target because object
presence is evaluated on a per-pixel basis.

\subsubsection{Recognition}
Object recognition is also important for target tracking, specifically if I want the tracker to be able to detect the target autonomously. Currently many of the best visual trackers require that the target be identified by a bounding box in the initial frame of the video.

However, for the proposed project, I do not assume that the target is necessarily in frame initially, or that a human operator is available to identify it. Rather, I would like to be able to provide a reference image to the program, and have it search for the target and find it if it appears in frame. Neural Networks have been used recently for vehicle recognition \cite{bautista2016convolutional, dong2014vehicle}, and it may be possible to use
these networks as "spotters" for target acquisition (given a vehicle type or a reference image).

Ideally, though, target acquisition is done through the same neural network that is used to track the object. Given that a CNN is already used in the model described by Hong \cite{hong2015online}, in may be possible that it would do target acquisition as well as tracking.

\newpage
\section{Research Goals}

\subsection{Motivation}
Recent research on autonomous target-tracking with quad-rotors has shown promising preliminary results \cite{pestana2014computer, woods2015dynamic, boudjit2015detection, lin2012robust}, however none of these represents a completely viable solution for several reasons. With the exception of \cite{lin2012robust}, all vision logic takes place on external hardware, rather than on an on-board computer. While this is fine for applications
where the drone can be communicated with, it adds the requirement that the drone be within range of whatever communication system is in use. It is also worth noting that communication over a network, wifi or otherwise, adds latency that can vary with bandwidth. It is possible that, with enough network lag, the on-board computer would not receive control updates in time for effective real-time tracking. Also, in \cite{lin2012robust,
woods2015dynamic} it should be noted that the targets are predetermined, however I desire a vision system that can track arbitrary targets based on a reference image.

Also, to my knowledge, there does not exist an implementation of small UAV target tracking using convolutional neural networks. All implementations that I could find rely, at least in part, on hand-coded features to track the target object, and often make assumptions about how the object can move. While some of these methods show impressive results \cite{kalal2012tracking, Nebehay2015CVPR} on the benchmarks they are tested against, they lack
the flexibility of pure learning algorithms, which can switch domains simply by using different sets of training data. Further, improving on the algorithms often requires coding ever more fine-grained models of the target to be tracked and, unlike trackers based on object localization models, can track only one target at a time.

I desire an architecture that makes as few assumptions as possible and is applicable to as many domains as possible.
\subsection{Problem Definition}

I aim to make a target-tracking and acquisition system that can be run on an on-board computer on a small, hex-rotor drone.

\subsection{Goals}

I break the problem into the following goals:

\begin{enumerate}
\item Create a vision-based, monocular target-tracking program that can be run on a small, low-power computer. This computer should draw a small enough amount of power, and be of dimensions such that it could conceivably be carried and run on-board a small rotary wing drone. The program should process video frames and return a bounding box around the target. The program should process frames
at a rate that allow the control system to effectively track the target, ideally at a rate $r \ge 20$ fps. It should also be robust to various common issues in target tracking: namely partial occlusion of target, object deformation and the target leaving and returning to frame.
\item Create a program similar to the one mentioned above, except that its implementation should use deep-learning to track the target. The purpose here is twofold: This implementation will be used as a feasibility test for deep-learning-based tracking on low-power devices. Assuming that this implementation can run in real time, it will also be used to compare the performance of a deep-learning-based target tracking with a more
conventional, feature-based tracker. While deep neural networks have been used successfully in target-tracking algorithms in recent years, they have, to the best of my knowledge, been tested exclusively on top-of-the-line, high-power GPUs, which are entirely unfeasible for deployment on small, battery-powered vehicles like copter drones. As stated above, an added performance constraint in this research is that the program be able to run on
a low-power device that is a feasible companion computer for a battery-powered vehicle.
%\item Optimize the deep-learning-based tracker for a specific, low-power architecture. Often, neural networks are designed with a specific number of cores in mind. Since most deep-learning tracking approaches are built for GPUs with a large number (1024) of cores, they're design may not be optimal for a smaller device.
\end{enumerate}

\newpage
\section{Methodology}

\subsection{Hardware}
\textbf{Onboard Tracking Computer:} I use the Nvidia Jetson TX1 SoC as the onboard computer for all target tracking and navigation purposes. The TX1 is ideal for this application. It consists of an ARM-64 CPU and a GPU of 256 CUDA cores. GPUs are essential for fast processing of neural networks. Neural networks require a large number of computations for each forward pass, however many of these can be done in parallel. Because GPUs consist of hundreds of cores, they can process neural networks much faster than conventional CPUs even when the clock rate of an individual core is significantly slower. Both the ARM architecture of the CPU and the Maxwell architecture of the GPU emphasize power efficiency. Even under a heavy load, the TX1 draws a maximum of 16W, while similar sized computers with CISC architectures can draw upwards of 50W. The power efficiency makes it perfect for battery powered applications. The TX1 and its motherboard are pictured in figures \ref{fig:SoC} and \ref{fig:devkit} respectively. I connected  a 60GB SSD drive to the TX1's SATA port for extra storage, and to support a swap file, which was necessary for building some libraries from source.

\noindent\textbf{Motherboard:} I use the Nvidia Jetson TX1 Carrier Board as the motherboard for the TX1. The board has a micro-ATX form factor, which makes it large for embedded applications. Nevertheless, it has ample ports that allow for a flexible development environment. Future work might attempt to use a smaller, more minimal motherboard to reduce size and weight.

\begin{figure}[h]
	\centering
	\begin{subfigure}{.5\textwidth}
  		\centering\captionsetup{width=.85\linewidth}
		\includegraphics[width=\linewidth]{img/nvidia_tx1.jpg}
		\caption{Nvidia TX1 System on a Chip}
		\label{fig:SoC}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering\captionsetup{width=.85\linewidth}%
		\includegraphics[width=\linewidth]{img/JTX1_devkit.png}
		\caption{Nvidia TX1 installed on the TX1 development kit carrier board}
		\label{fig:devkit}
	\end{subfigure}%
	\caption{TX1 Development Kit and SoC}
	\label{fig:tx1}
\end{figure}

\noindent\textbf{Flight Controller:} For the flight controller, I use a 3DR Pixhawk (figure \ref{fig:pixhawk}). The target-tracking computer (TX1) and the Pixhawk communicate over a serial connection using the MAVLink protocol. The target-tracking computer sends navigation directions via the serial connection, while the pixhawk sends status updates at regular intervals.

\noindent\textbf{Camera:} I use a Logitech C310 for image capture (figure \ref{fig:logitech}). Images are transmitted to the target-tracking computer via USB.

\begin{figure}[h]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.85\linewidth]{img/pixhawk.jpg}
		\caption{3DR Pixhawk}
		\label{fig:pixhawk}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.85\linewidth]{img/logitech.jpeg}
		\caption{Logitech C310}
		\label{fig:logitech}
	\end{subfigure}%
	\caption{Pixhawk and Logitech C310 camera}
	\label{fig:camhawk}
\end{figure}

\noindent\textbf{DJI S900:} The intended drone for this application is a DJI S900 hexacopter (figure \ref{fig:drone}), although it may be used with any drone assuming the Pixhawk is properly calibrated.

\begin{figure}[h]
	\centering
	\includegraphics[width=.6\textwidth]{img/dji.jpg}
	\caption{DJI S900 hexacopter}
	\label{fig:drone}
\end{figure}

\subsection{Software Stack}
\noindent\textbf{Environment:} All code for this project was written in Python to run on the Nvidia Jetson TX1. The operating system is the Linux for Tegra (L4T) variant of Ubuntu 14.04 (Trusty Taur). I used the Python 2.7 runtime environment, rather than 3.x, because the source code for the tracking algorithms is available only in 2.7. Prior to development, the TX1 was also flashed with Nvidia Jetpack 2.3.1. Jetpack is a disk image that packages the following tools and libraries: CUDA \cite{nickolls2008scalable} 8.0, Nvidia's API to for parallel programming on their GPUs; cuDNN 7.0, a CUDA library for speeding up neural networks on Nvidia GPUs.

\noindent\textbf{CMT:} The authors of \cite{Nebehay2015CVPR} make the code for their paper available on Github \cite{cmt}. We use it to test the feasibility of a feature-based tracker.

\noindent\textbf{SiamFC:} The authors of \cite{bertinetto2016fully} make a Python port (the original code is in Matlab) of their algorithm available on Github \cite{siamfc}. We use it to test the feasibility of a deep-learning-based tracker.

\noindent\textbf{OpenCV:} OpenCV \cite{opencv_library}, an open source computer vision library, is a dependency of the Python implementation of the CMT algorithm, which is used for one of the tracking algorithms. OpenCV also provides convenient and fast methods for displaying images, which are used for visualizing tracking performance.

\noindent\textbf{Tensorflow v0.11:} Tensorflow \cite{tensorflow2015-whitepaper} is an open source computational graph library developed and maintained by Google. It is used widely as a library for building, training and running neural networks, and is a dependency for the Python implementation of the SiamFC algorithm, which is used as one of the tracking algorithms.

\noindent\textbf{NumPy:} NumPy (short for numeric Python) \cite{walt2011numpy} is a python library for matrix and tensor operations. It also provides packages for reading images from files that are used in this work for running the Online Tracking Benchmark \cite{wu2013online}.

\noindent\textbf{APSync:} Apsync \cite{apsync} is a suite of scripts for onboard computers that allow for data logging, data transfer, and rapid remote access over wifi. Apsync is used in this research as a way to connect with the TX1 while in flight.

\noindent\textbf{MAVLink:} MAVLink \cite{mavlink} is a library for lightweight messaging between or to and from drones. It supports several languages. The Python bindings are used in this research to transmit messages between the Pixhawk and the TX1.

\noindent\textbf{Note:} The Jetson TX1 is not, as yet, well supported in python. Both OpenCV and Tensorflow had to be built from source, and required several patches in order to make them run properly on the TX1. In the case of Tensorflow 0.11, some knowledge of the build tool Bazel is required, as several of the dependency URLs are out of date. The Tensorflow build also requires more physical memory (around 10GB) than is available on the TX1 (8GB). In order to provide the requisite memory, a SATA SSD was installed on the TX1 and a swap file added. The scripts provided by the Jetsonhacks \cite{jetsonhacks2016} repository on Github served as a starting point for the Tensorflow build. It should be noted that the more recent commits update the Tensorflow version to 1.3, which is current as of this writing. The commit referenced in the notes gives the state of the files when they were used for this work (and install Tensorflow 0.11).

\subsection{Implementation}
\noindent\textbf{Hardware Setup:} The logitech C310 camera is connected to the TX1 via the carrier board's USB port. The Pixhawk is connected to the TX1 via a UART serial connection. The serial wires run from the UART pins on the carrier board's expansion header to the "telemetry-2" port on the Pixhawk. It's important to note that the power serial wire is NOT used, as this can damage to the Pixhawk. Only the ground, transmit, receive, request-to-send, and clear-to-send pins should be connected to the Pixhawk's telemetry-2 port (These correspond to pins 6, 8, 10, 11 and 36 on the carrier board's expansion header, respectively). The power wire should be disconnected (or absent). The Pixhawk is powered externally by the battery.

\noindent\textbf{Software:} The main function of the tracking algorithm implemented for this research can be described by the following algorithm:

\begin{algorithm}
\caption{Main}
\label{alg:thealg}
\begin{algorithmic}[1]
	
	\State $frame \gets$ current image from camera feed
	\State $position \gets$ target position from user
	\State initialize tracking algorithm with $frame$ and $position$\\
	
	\While{User has not hit stop key}
		\State $frame \gets$ current image from camera feed
		\State $position \gets$ result of tracking algorithm on $frame$
		\State $velocity \gets$ new velocity vector calculated from $position$
		\State update drone velocity and heading with $velocity$
	\EndWhile\\
	
\end{algorithmic}
\end{algorithm}

\noindent
The first two lines of the algorithm are fairly trivial. OpenCV provides an API for reading images from files, which makes reading from a camera simple, since UNIX systems (like Ubuntu) treat all devices as files. OpenCV also provides an API for drawing shapes as well as for tracking mouse movement, which the code in \cite{Nebehay2015CVPR} uses to make a simple GUI for the user to draw a bounding box around the target, and which I use in the tracking code. The navigation logic is described in section \ref{navigation} and the two tracking implementations are discussed in sections \ref{goal1} and \ref{goal2}. In the case of both trackers, the tracking algorithm accounts for the bulk of the time of each iteration.

\subsubsection{Navigation}\label{navigation}
For the purposes of this paper, a very simple navigation logic was implemented. The task of the navigation logic is to keep the drone following the target at a constant speed and constant altitude. At each iteration of the while loop in algorithm~\ref{alg:thealg}, the tracker returns a new position for the target. This position is in the form of a "bounding box" around the target. A bounding box, as I use the term in this thesis, is a rectangle that is normal to the image axes. An example bounding box is shown in figure~\ref{fig:bbox}. Since it is presumed that the tracker has fit the bounding box tightly to the target, the algorithm presumes that the target is more or less at the center of the bounding box. To keep the drone tracking the target, the navigation logic finds the centroid of the bounding box in image coordinates, and then converts that position into a vector extending from the front of the drone towards the target. A monocular vision system cannot directly calculate the distance to a target in general. However, for the purposes of this research we presume the target is ground-based. Using this information, and assuming that the ground is approximately planar, the navigation logic computes the point of intersection between the ray extending from the camera towards the target with the plane of the ground, and assumes that to be the target position in relative coordinates. Then, using the method described in \cite{cai2011coordinate}, the relative coordinates are converted to north-east-down (NED) coordinates. The down coordinate is removed and the remaining north-east vector is scaled to have a magnitude of 5 m/s. Similarly, we calculate the new heading of the drone as:
\[ yaw = \arctan \Big( \frac{east}{north} \Big) \times \frac{180}{\pi} \]
The velocity vector and the yaw are then passed via MAVLink to the Pixhawk, which implements the control logic to change the drone's velocity and heading.

\begin{figure}
	\centering
	\includegraphics{img/bbox.png}
	\caption{Example of a bounding box around a target. Figure Credit \cite{wu2013online}}
	\label{fig:bbox}
\end{figure}

\subsubsection{Feature-based tracker} \label{goal1}
I use the CMT algorithm described in \cite{Nebehay2015CVPR} as a feature-based learning tracking implementation. The results of this tracker show good performance, it is capable of running at a near-real-time frame rate (10 fps) on the Jetson TX1, and the code is openly available, so it should be easy to implement. 

\subsubsection{Learning-based tracker} \label{goal2}
I use the network architecture described in \cite{bertinetto2016fully} as the basis for a deep-learning based tracker. This algorithm is promising because it doesn't require computationally expensive operations like back-propagation, or learning an online model, as in other CNN-based trackers. Rather, \cite{bertinetto2016fully} uses a pair of CNNs called, collectively, a Siamese network. The basic concept behind Siamese networks is to use two identical networks to extract image features on both a candidate image and a reference image, and then use a simple distance metric to compare the representations of the images in feature space. The candidate image that minimizes that distance is considered to be the new target location. This approach is attractive for several reasons. First off, it is exceedingly simple. Since the reference image never changes, we may compute its features exactly once at the beginning of tracking. From that point on, each tracking step involves simply passing candidate images through the same network, and comparing the results to the reference features. As Bertinetto and Valmadre show, the benefit of such a simple architecture is a vastly improved frame rate. Specifically, they claim to achieve frame rates of around 85 fps, a great leap forward compared to other CNN-based trackers, which appear to hover at or below 15fps.

%\subsection{Improvements to the learning-based tracker} \label{goal3}
%It is possible, even likely, that a neural network designed with a 1024-core GPU in mind will not run particularly well on a single 256-core GPU. In that case, we may expect to see significant improvements to performance if the CNN architecture is changed to take into account the smaller number of cores. Furthermore, it is of note that Bertinetto uses the convolutional architecture of "Alexnet" as described in
%\cite{krizhevsky2012imagenet}. A particular feature of Alexnet that may have room for improvement is the size of its filters. the Alexnet filters start at size ($1 \times 11$, then proceeds to convolutions of $5 \times 5$, and then $3 \times 3$ for the final three convolutional layers. However, there is a strong argument for using smaller filters, and making up for the spatial loss by using a deeper network. Consider the fact that
%a $7 \times 7$ filter has the same receptive field (area of the image that actually affects the output neuron at a given location) as $3  \times 3$ convolutional layers. However, the smaller filters will also incorporate more nonlinearity, and decrease both the number of parameters to learn (49 vs. 27) and the number of floating-point multiplies needed for a forward pass - all of which suggest improved performance and quality.
%My suggeston then, is to replace the large filters in the early stages of the Alexnet architecture with several smaller layers, and compare performance with the original implementation.
%However, changing the architecture means that I can no long take advantage of "transfer learning," the practice of using weights from one learned model (e.g. Alexnet), and transferring them to a different application with the same network structure. In this case, I will train the convolutional network myself, using the same strategy as described in \cite{bertinetto2016fully}.

\newpage
\section{Results}

\subsection{Deliverables}

\textbf{Tracking Module:} A target tracking module comprised of a Jetson TX1 embedded device with runnable target-tracking software installed. Currently, the tracking program is written to follow a target at constant speed until it is no longer in frame, however this behavior could be modified. Due to flight equipment being unavailable, the software has not yet been tested end-to-end. However the two primary modules of the software (visual target-tracker and flight controller) have individually passed functionality tests. The flight controller has shown, in simulation, that it will correctly follow a target within its view, and, as evidenced by the tracking results in \ref{trackereval} Remote connection to the TX1 is handled over wifi, which can be used to launch the tracking script and set the target.

\textbf{Application}: The tracking application, consisting of a library of visual target-trackers and a flight controller for following a target, has been made freely available via github. Documentation has been provided to assist in using the tracking module, and it is my sincere hope that someone will actually try to use it.

\subsection{Evaluation of Target trackers}\label{trackereval}

All trackers are evaluated against the Online Tracking Benchmark (OTB) developed by Yi Wu, Jongwoo Lim and Ming-Hsuan Yang in \cite{wu2013online}, and compared against each other. To the best of my knowledge, OTB is the most thorough, complete, and widely used benchmark for online tracking. Further, the authors of the SiamFC and CMT algorithms (\cite{bertinetto2016fully} and \cite{Nebehay2015CVPR}, respectively) both use OTB to evaluate their results, and using it again here provides a measure of continuity with their research.

OTB uses two metrics of prediction at each frame in a sequence, precision and intersection-over-union (IoU). In both cases, the metrics are compared against an arbitrary threshold and are considered to pass if they meet or exceed the expectations defined by the threshold, and fail otherwise.

Precision is simply the Euclidean distance between the centers of the predicted and ground-truth bounding boxes. For a given frame, the tracking algorithm is considered to have been successfully precise if that distance is $d \leq \theta_p$ where $\theta_p$ is an arbitrary threshold, usually in the range $[0, 50]$.

Precision measures how centered the bounding box is on the target, but contains no information about how well the bounding box fits the target. Precision does not penalize a predicted bounding box for being larger or smaller than the target. For this reason, OTB uses IoU and as its primary metric. IoU is defined as the area of overlap (intersection) of the predicted and ground-truth bounding boxes, divided by the total area
that they cover (union). That is, for a predicted bounding box with area $A_p$ and a ground-truth bounding box with area $A_t$, the IoU is defined by:

\begin{gather*}
\frac{A_p \cap A_t}{A_p \cup A_t}
\end{gather*}

A tracker is successful on a given frame if the IoU $\geq \theta_s$ where $\theta_s$ is an arbitrary threshold in the range $[0,1]$. For the purposes of this project I report only the IoU, and not the precision.

Each tracker receives a single IoU score for each test in the tracking benchmark (there are three, as explained later). In the simplest case, this is calculated by computing the IoU for each frame in the 100 sequences in the OTB-100 dataset, and then calculating the frequency of success at 50 evenly spaced thresholds in the range $[0,1]$. This generates a plot of success rate as a function of threshold value. The final IoU score is the area under the curve (AUC) of the IoU success rate plot.

OTB uses three separate tests, each of which generates one precision plot and one success plot. One-Pass Evaluation (OPE), Spatial Perturbation Evaluation (SPE), and Temporal Perturbation Evaluation (TPE) to evaluate the performance of online trackers.

The OPE test runs the tracker over each sequence exactly once. The tracker is given the ground-truth bounding box for the target in the first frame, and generates predicted bounding boxes for each frame thereafter. As the authors of OTB note in \cite{wu2013online}, one-pass evaluation may under- or overestimate the performance of a tracker if the initial frames and positions of the targets are, on average, particularly favorable or unfavorable for the tracker being evaluated. In order to evaluate a tracker's robustness to changes in start frame and target selection, the OTB benchmark implements the TPE and SPE tests respectively. The SPE test runs the tracker on each sequence 12 times; for each run, the initial bounding box is slightly scaled or shifted to mimic variance in user-defined bounding boxes. Similarly, the TPE test breaks each sequence into twenty sub-sequences, and runs the tracker over each subsequence, re-initializing the tracker with the first frame and ground-truth bounding box of each. As with OPE, the frame-wise metrics are computed at different thresholds, and the AUC computed.

Speed is reported in frames per second (fps).

\subsection{Environment}

All tracking algorithms were run on a Jetson TX1 running Linux for Tegra (L4T) Ubuntu 14.04 (Trusty Taur). All algorithms were implemented in python, and were modified minimally from the original code. Although the TX1 has 3.9 GiB of memory, only around 2.43 GiB memory is available in user space. L4T Ubuntu is a multi-tasking OS, and is not the most efficient environment in which a tracker can run. However, it mimics the real environment in which a tracker would have to run on small drone, since the onboard computer would have to handle several tasks (e.g. communicating with ground control).

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering\captionsetup{width=.85\linewidth}%
  \includegraphics[width=\linewidth]{img/cmt_siamfc_AUC.png}
  \caption{OPE plot for the CMT and SiamFC trackers on a Jetson TX1}
  \label{fig:ope_vs}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering\captionsetup{width=.85\linewidth}
  \includegraphics[width=\linewidth]{img/all_trackers_AUC.png}
  \caption{OPE plot for several trackers, SiamFC is the best performing.}
  \label{fig:ope_all}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering\captionsetup{width=.85\linewidth}%
  \includegraphics[width=\linewidth]{img/sre_vs.png}
  \caption{SRE plot for the CMT and SiamFC trackers on a Jetson TX1}
  \label{fig:sre_vs}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering\captionsetup{width=.85\linewidth}
  \includegraphics[width=\linewidth]{img/sre_all.png}
  \caption{SRE plot for several trackers, SiamFC is the best performing.}
  \label{fig:sre_all}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering\captionsetup{width=.85\linewidth}%
  \includegraphics[width=\linewidth]{img/tre_vs.png}
  \caption{TRE plot for the CMT and SiamFC trackers on a Jetson TX1}
  \label{fig:tre_vs}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering\captionsetup{width=.85\linewidth}
  \includegraphics[width=\linewidth]{img/tre_all.png}
  \caption{TRE plot for several trackers, SiamFC is the best performing.}
  \label{fig:tre_all}
\end{subfigure}
\caption{Online Tracking Benchmark results for the CMT and SiamFC trackers. Note: There are trackers that outperform the SiamFC algorithm in these tests, however, they were not available in the benchmark implementation at the time of this writing.}
\label{fig:ope}
\end{figure}


\subsection{Tracking Results}\label{tracking_results}

Figure \ref{fig:ope} shows the results of the OPE test. As is evident from plot \ref{fig:ope_vs}, the SiamFC algorithm far outperforms the CMT algorithm; and plot \ref{fig:ope_all} shows that SiamFC outperforms several other well-known feature-based tracking algorithms. However, the SiamFC algorithm ran at an average of 2.21 fps over the OPE test, while the CMT algorithm ran at an average of 10.70 fps. Notably, the SiamFC tracker did not have enough memory to store the entire computational graph in memory, which likely adversely affected its speed.

\newpage
\section{Conclusion and Future Work}

\subsection{Conclusion}
In this thesis I have implemented and demonstrated the feasibility of a tracking module for small, low-power devices. The CMT tracker coupled with sound control logic, could conceivably be used to autonomously track an object. I have also shown that a learning-based algorithm, such as the SiamFC algorithm, can be run on a low-power computer such as the Nvidia Jetson TX1. Although its performance was too slow to currently be considered for online tracking, it is not inconceivable that, with some optimization, it could reach real-time speed. This would allow small autonomous devices to take advantage of the powerful computer vision applications of neural networks.

\subsection{Future Work}
Several questions are left unanswered by this research. Foremost is the question of whether a neural network large enough to perform at or near state-of the art tracking performance (i.e. on the OTB benchmark) can be run in real time on a device as small as the Jetson TX1. Given that lack of sufficient memory for the network is the most likely culprit for the slow speed of the SiamFC algorithm on the TX1, reducing the memory load of the algorithm is an obvious first step.

A couple of options exist for reducing memory load. The first is to change the network architecture. The authors of the SiamFC algorithm used convolutional layers inspired by Alexnet in \cite{krizhevsky2012imagenet}. However, as shown in \cite{simonyan2014very}, a deeper network architecture with smaller convolutional kernels can simultaneously decrease the number of parameters in the network, and increase performance. Perhaps using an updated architecture could both reduce memory load without decreasing (and possibly increasing) tracking performance.

A second approach would be to keep the same basic architecture (i.e. kernel size and depth) while reducing the total number of kernels in each layer.

Because these changes occur along different axes (kernel-size and depth, and kernel count), they can be combined. For example, we may use the first 10 convolutional layers of VGGNet while reducing the number of kernels in each layer by half.

Another obvious solution to the memory problem would be to simply increase the available memory on the device. As a research goal this is not of much interest, but it would serve to validate whether having the whole network in memory significantly increases performance and, if so, by how much.

There is also ample room for further research in the domain of control logic. The resulting module of this research uses a fixed camera that is mounted on the logical "front" of the copter drone, which limits its utility. Ideally, the module would use a camera mounted on a gimbal and implement two control logics: one for the camera, and one for the drone itself. An ideal control situation would be one that keeps the camera tracking the object of interest, while drone itself is free to navigate an unrelated pattern (e.g. circling the target, or executing maneuvers for collision avoidance).

\newpage
\bibliography{references}
\bibliographystyle{ieeetr}
\end{document}
}})
